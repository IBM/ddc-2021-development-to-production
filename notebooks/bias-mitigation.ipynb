{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Understand and Reduce Bias in Machine Learning"}, {"metadata": {}, "cell_type": "markdown", "source": "Building accurate machine learning models is not always good enough, especially when predictions are used to make decisions that impact peoples lives. Models trained using supervised learning techniques attempt to find patterns or generalizations in a training dataset, then use those patterns to make predictions. These patterns may sometimess be undesirable or even illegal. For example, a loan risk model may determine that age plays a significant role in the prediction of a loan being deemed risky because the training dataset happened to have more risk for one age group than for another. \n\nThis notebook gives practical examples to define and quantify the fairness of both data and models, exploring algorithms to detect bias and disparity in data, and mitigate bias in both data and models."}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.0 Bias Definitions"}, {"metadata": {}, "cell_type": "markdown", "source": "Although bias can have multiple definitions in the context of machine learning (i.e [**bias error where models miss relevant relations between features and targets**](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), [**bias of an estimator**](https://en.wikipedia.org/wiki/Bias_of_an_estimator), etc). There are certain types of bias that can cause unfairness:\n\n* **Selection bias** involves individuals being more likely to be selected for study than others, biasing the sample\n* **Spectrum bias** arises from evaluating diagnostic tests on biased patient samples, leading to an overestimate of the sensitivity and specificity of the test\n* The **bias of an estimator** is the difference between an estimator's expected value and the true value of the parameter being estimated\n* **Omitted-variable bias** is the bias that appears in estimates of parameters in regression analysis when the assumed specification omits an independent variable that should be in the model\n* **Detection bias** occurs when a phenomenon is more likely to be observed for a particular set of study subjects. \n* **Funding bias** may lead to the selection of outcomes, test samples, or test procedures that favor a study's financial sponsor.\n* **Reporting bias** involves a skew in the availability of data, such that observations of a certain kind are more likely to be reported.\n* **Analytical bias** arises due to the way that the results are evaluated.\n* **Exclusion bias** arise due to the systematic exclusion of certain individuals from the study.\n* **Attrition bias** arises due to a loss of participants e.g. loss to follow up during a study.\n* **Recall bias** arises due to differences in the accuracy or completeness of participant recollections of past events. e.g. a patient cannot recall how many cigarettes they smoked last week exactly, leading to over-estimation or under-estimation.\n* **Observer bias** arises when the researcher subconsciously influences the experiment due to cognitive bias where judgment may alter how an experiment is carried out / how results are recorded.\n\nFor fair models, metrics are needed that can help explore if any of the above biases is present in data and models. "}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.0 AI Fairness Toolkit\n\nThroughout this notebook the [AI Fairness 360](https://github.com/Trusted-AI/AIF360) toolkit will be used. AI Fairness 360 (AIF360) is an extensible, open source toolkit for measuring, understanding, and removing AI bias. It contains the most widely used bias metrics, bias mitigation algorithms, and metric explainers from the top AI fairness researchers across industry & academia. The aif360 Python package includes a comprehensive set of metrics for datasets and models to test for biases, explanations for these metrics, and algorithms to mitigate bias in datasets and models. Find more resources and an interactive demo [here](http://aif360.mybluemix.net/)."}, {"metadata": {}, "cell_type": "markdown", "source": "### Import Packages"}, {"metadata": {}, "cell_type": "code", "source": "!pip install --upgrade 'aif360[all]' --no-cache | tail -n 1\n!pip install --upgrade cvxpy --no-cache | tail -n 1\n!pip install --upgrade tqdm --no-cache | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import warnings\nwarnings.filterwarnings('ignore')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import sys\nsys.path.insert(1, \"../\")  ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nnp.random.seed(0)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nfrom aif360.datasets import GermanDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric\nfrom aif360.metrics import ClassificationMetric\nfrom aif360.algorithms.preprocessing import Reweighing\nfrom aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\nfrom aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_german\nfrom aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions import get_distortion_german\nfrom aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\nfrom aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n\nfrom IPython.display import Markdown, display\n%matplotlib inline", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3.0 Load and Explore Data\n\nA [dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29) that classifies people described by a set of attributes as good or bad credit risks.\n\nThis data is one of the example [datasets](https://aif360.readthedocs.io/en/latest/modules/datasets.html#module-aif360.datasets) used in aif360 and has it's own [class](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.GermanDataset.html#aif360.datasets.GermanDataset) that will be used. \n\n### Load data\n\nIt is assumed that the dataset can be found within a specific location. Let's create this folder and the download the data to this new folder. The advantage is that this works both locally and when running in Watson Studio."}, {"metadata": {}, "cell_type": "code", "source": "aif360_location = !python -c \"from distutils.sysconfig import get_python_lib; print(get_python_lib())\"\nimport os\ninstall_loc = os.path.join(aif360_location[0], \"aif360/data/raw/german/\")\n%cd $install_loc", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!wget ftp://ftp.ics.uci.edu/pub/machine-learning-databases/statlog/german/german.data\n!wget ftp://ftp.ics.uci.edu/pub/machine-learning-databases/statlog/german/german.doc\n%cd -", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dataset_german = GermanDataset()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### AIF 360 Data Format\n\nAll variables of this dataset are described in the [documentation](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.GermanDataset.html) with more details in the description of the [`StandardDataset`](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.StandardDataset.html). In short, the dataset class contains a numpy array or pandas DataFrame with several variables. "}, {"metadata": {}, "cell_type": "code", "source": "print(f'Features Type: {type(dataset_german.features)}')\nprint(f'Labels: {dataset_german.label_names}')\nprint(f'Protected Attributes: {dataset_german.protected_attribute_names}')\nprint(f'Number of Features: {len(dataset_german.feature_names)}')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(f'Feature Names: {dataset_german.feature_names}')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Explore with pandas\n\nConvert the data to a `features` DataFrame and `labels` Series.\n\nRead more about the data and its attributes [here](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29)."}, {"metadata": {}, "cell_type": "code", "source": "features = pd.DataFrame(dataset_german.features, columns=dataset_german.feature_names)\nlabels = pd.Series(dataset_german.labels.ravel(), name=dataset_german.label_names[0])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "features.describe().transpose().head(55)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Feature Distribution"}, {"metadata": {}, "cell_type": "code", "source": "plt.rcParams[\"figure.figsize\"] = (18,18)\n\nfeatures.hist();\nplt.tight_layout()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4.0 Exploring Bias with AIF360\n\nWhen considering a model created in a supervised machine learning process:\n\n<img src=\"https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/images/Complex_NoProc_V3.jpg\"   width=\"500\" align=\"left\">\n"}, {"metadata": {}, "cell_type": "markdown", "source": "Bias can enter the system in any of these three steps:\n1. The process starts with a training dataset, which contains a sequence of instances, where each instance has two components: the features and the correct prediction for those features.\n  * The training data set may be biased in that its outcomes may be biased towards particular kinds of instances\n2. A machine learning algorithm is trained on this training dataset to produce a machine learning model. This generated model can be used to make a prediction when given a new instance.\n  * The algorithm that creates the model may be biased in that it may generate models that are weighted towards particular features in the input\n3. A second dataset with features and correct predictions, called a test dataset, is used to assess the accuracy of the model. Since this test dataset is the same format as the training dataset, a set of instances of features and prediction pairs, often these two datasets derive from the same initial dataset. A random partitioning algorithm is used to split the initial dataset into training and test datasets.\n  * The test data set may be biased in that it has expectations on correct answers that may be biased\n\nThese three points in the machine learning process represent points for testing and mitigating bias. In AI Fairness 360 these are called:\n* pre-processing \n* in-processing\n* post-processing\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### Bias Metrics and Definitions\n\nTo help understand bias, the AIF360 toolkit provides more than 70 metrics. The metrics you might choose to measure fairness will vary based on the use case or application being built, as well as where in the machine learning pipline you want to measure (i.e the training data or the model). [This demo](http://aif360.mybluemix.net/data) provides explanations and definitions of the metrics as well. Some intial guidance based on how you want to measure fairness:\n\n- **Group Fairness**: Partitions a population into groups defined by protected attributes and seeks for some statistical measure to be equal across groups. For group fairness you might choose to use the metrics in the [DatasetMetric class](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.DatasetMetric.html). \n- **Individual Fairness**: Seeks for similar individuals to be treated similarly. For individual fairness you might choose to use the metrics in the [SampleDistortionMetric class](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.SampleDistortionMetric.html)\n- **Both**: When considering both group and individual fairness, you might choose to explore the generalized entropy index and its specializations to Theil index and coefficient of variation in the [ClassificationMetric class](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html)\n\n\nBefore we calculate the metrics, some quick definitions:\n\n* `mean_difference` (alias of `statistical_parity_difference`) - The difference of the rate of favorable outcomes received by the unprivileged group to the privileged group. A negative value indicates less favorable outcomes for the unprivileged groups. Although an ideal value of this metric is 0, fairness for the metric is between -0.1 and 0.1\n\n* `disparate_impact` - The ratio of rate of favorable outcome for the unprivileged group to that of the privileged group\n     \n* `consistency` - The individual fairness metric that measures how similar the labels are for similar instances.\n\n* `equal_opportunity_difference` (alias of `true_positive_rate_difference`) \n    * true_positive_rate: ratio of true positives to positive examples in the dataset\n\n* `base_rate` - The number of positives divided by number of positives plus negatives\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1 Bias in Credit Dataset (Training Data)\n\nIn the dataset we are exploring, bias could occur based on age or sex. To explore the age bias:\n\n* set the protected attribute to be `age`, where `age >=25` is considered privileged\n* the protected attribute for `sex` is not consider in this evaluation\n* split the original dataset into training and testing datasets\n* set two variables for the privileged (1) and unprivileged (0) values for the age attribute. These are key inputs for detecting and mitigating bias\n\n\n\nThe protected attribute will be \"Age\", with \"1\" (older than or equal to 25) and \"0\" (younger than 25) being the values for the privileged and unprivileged groups, respectively."}, {"metadata": {}, "cell_type": "markdown", "source": "#### Metrics based on a single `BinaryLabelDataset`\n\nSince the dataset we are exploring has two values of the label, we can look for [binary dataset label metrics](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.BinaryLabelDatasetMetric.html)"}, {"metadata": {}, "cell_type": "code", "source": "dataset_german = GermanDataset(protected_attribute_names=['age'],\n                    privileged_classes=[lambda x: x >= 25],      \n                    features_to_drop=['personal_status', 'sex']) \n\n# Split into train, validation, and test\ndataset_german_train, dataset_german_test = dataset_german.split([0.7], shuffle=True)\n\n# Age: 1 are those older than or equal to 25. 0 is younger than 25\nprivileged_groups = [{'age': 1}]\nunprivileged_groups = [{'age': 0}]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "metric_german_train = BinaryLabelDatasetMetric(dataset_german_train, \n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\n\nmetric_german_test = BinaryLabelDatasetMetric(dataset_german_test, \n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\n\n#help(metric_german_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(\"Metrics for Original Training Dataset:\")\nprint(\"\\tDifference in mean outcomes between unprivileged and privileged groups = %f\" % metric_german_train.mean_difference())\nprint(\"\\tdisparate_impact = %f\" % metric_german_train.disparate_impact())\nprint(\"\\tconsistency = %f\" % metric_german_train.consistency())\nprint(\"\\tbase_rate = %f\" % metric_german_train.base_rate())\nprint(\"\\tnum_negatives = %f\" % metric_german_train.num_negatives())\nprint(\"\\tnum_positives = %f\" % metric_german_train.num_positives())\n\n#print(\"Metrics for Original Test dataset:\")\n#print(\"\\tmean_difference = %f\" % metric_german_test.mean_difference())\n#print(\"\\tdisparate_impact = %f\" % metric_german_test.disparate_impact())\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.2 Bias in Credit Model (ML Algorithm)\n\nBefore we can explore the bias in the machine learning algorithm. We need to build the model. When we explored the dataset we have loaded, you may have noticed:\n- It is fairly clean and tidy (no missing values)\n- It is already one-hot encoded for multiple classes\n\nThe only thing we really need to do is scale and normalize the features. For this we can use [StandardScaler](https://scikit-learn.org/stable/modules/preprocessing.html) - \n*Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. `StandardScaler` implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set.*\n\naif360 format can be used with scikitlearn!"}, {"metadata": {}, "cell_type": "code", "source": "scale_german = StandardScaler().fit(dataset_german_train.features)\n\nX_train = scale_german.transform(dataset_german_train.features)\ny_train = dataset_german_train.labels.ravel()\nw_train = dataset_german_train.instance_weights.ravel()\n\nX_test = scale_german.transform(dataset_german_test.features)\ny_test = dataset_german_test.labels.ravel()\nw_test = dataset_german_test.instance_weights.ravel()\n\n# plt.rcParams[\"figure.figsize\"] = (18,18)\n# scaled_features = pd.DataFrame(X_train, columns=dataset_german.feature_names)\n# scaled_features.hist();\n# plt.tight_layout()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Build Model Using Original Training Data\n\nThere are a variety of algorithms we could use for this binary classification scenario. For this exercise, we will simply use Logistic Regression and then use the model to make predicitons on the test dataset."}, {"metadata": {}, "cell_type": "code", "source": "# Logistic regression classifier and predictions\n\n# create an instance of the model\nlmod = LogisticRegression()\n\n# train the model\nlmod.fit(X_train, y_train, \n         sample_weight=dataset_german_train.instance_weights)\n\n# calculate predicted labels\ny_train_pred = lmod.predict(X_train)\n\n# assign positive class index\npos_ind = np.where(lmod.classes_ == dataset_german_train.favorable_label)[0][0]\n\n# add predicted labels to predictions dataset\ndataset_german_train_pred = dataset_german_train.copy()\ndataset_german_train_pred.labels = y_train_pred\n\ndataset_german_test_pred = dataset_german_test.copy(deepcopy=True)\nX_test = scale_german.transform(dataset_german_test_pred.features)\ny_test = dataset_german_test_pred.labels\ndataset_german_test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Model Performance\n"}, {"metadata": {}, "cell_type": "code", "source": "score = lmod.score(X_test, y_test)\nprint(\"Logistic Regression model accuracy = %f\" % score)\n\n# confusion matrix\ncm = metrics.confusion_matrix(y_test, lmod.predict(X_test))\n\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\nplt.ylabel('Actual label');\nplt.xlabel('Predicted label');", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_test = pd.DataFrame(dataset_german_test.features, columns=dataset_german_test.feature_names)\n\n[fig, ax] = plt.subplots(1,2, figsize=(15, 5));\nplot_confusion_matrix(lmod, X_test[df_test['age']==0], y_test[df_test['age']==0],\n                      cmap=plt.cm.Blues, \n                      display_labels=['good credit','bad credit'],\n                      ax=ax[0]);\nax[0].set_title('Age < 25')\n\nplot_confusion_matrix(lmod, X_test[df_test['age']==1], y_test[df_test['age']==1],\n                      cmap=plt.cm.Blues, \n                      display_labels=['good credit','bad credit'],\n                      ax=ax[1]);\nax[1].set_title('Age > 25');", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Metrics based on the `ClassificationMetric class`\n\nSince we have built a classification model, we can explore the [`ClassificationMetric class metrics`](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html) for bias in predictions\n"}, {"metadata": {}, "cell_type": "code", "source": "# Calculate the metrics on both the training and test data\nmetric_german_train_pred = ClassificationMetric(dataset_german_train, dataset_german_train_pred,\n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\n\nmetric_german_pred = ClassificationMetric(dataset_german_test, dataset_german_test_pred,\n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(\"Predicted labels (training dataset):\")\nprint(\"\\tmean_difference = %f\" % metric_german_train_pred.mean_difference())\nprint(\"\\tdisparate_impact = %f\" % metric_german_train_pred.disparate_impact())\n\nprint(\"Predicted labels (test dataset):\")\nprint(\"\\tmean_difference = %f\" % metric_german_pred.mean_difference())\nprint(\"\\tdisparate_impact = %f\" % metric_german_pred.disparate_impact())\n\nprint(\"Quality metrics  (training dataset):\")\nprint(\"\\taccuracy = %f\" % metric_german_train_pred.accuracy())\nprint(\"\\tcoefficient_of_variation = %f\" % metric_german_train_pred.coefficient_of_variation())\nprint(\"\\terror_rate = %f\" % metric_german_train_pred.error_rate())\n\nprint(\"Quality metrics  (test dataset):\")\nprint(\"\\taccuracy = %f\" % metric_german_pred.accuracy())\nprint(\"\\tcoefficient_of_variation = %f\" % metric_german_pred.coefficient_of_variation())\nprint(\"\\terror_rate = %f\" % metric_german_pred.error_rate())\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 5.0 AI Fairness Algorithms\n\nThere are three points in the machine learning process where testing and mitigating bias can be done. In AI Fairness 360 these are called:\n- pre-processing (i.e mitigation happens before the creation of the model).\n- in-processing\n- post-processing"}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.1 Pre-processing algorithms\n\nThe previous sections showed that the privileged group was getting 17% more positive outcomes in the training dataset. Since this is not desirable, we are going to try to mitigate this bias in the training dataset. AI Fairness 360 implements several pre-processing mitigation algorithms. \n\nWe will choose the Reweighing algorithm [1], which is implemented in the Reweighing class in the aif360.algorithms.preprocessing package. This algorithm will transform the dataset to have more equity in positive outcomes on the protected attribute for the privileged and unprivileged groups. We then call the fit and transform methods to perform the transformation, producing a newly transformed training dataset (dataset_transf_train).\n\n[1] F. Kamiran and T. Calders, \"Data Preprocessing Techniques for Classification without Discrimination,\" Knowledge and Information Systems, 2012."}, {"metadata": {}, "cell_type": "markdown", "source": "#### Remove Bias by Reweighing Data\n\n**Reweighing** is a preprocessing technique that weights the examples in each (group, label) combination differently to ensure fairness before classification. Read the [documentation](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html) for a full overview."}, {"metadata": {}, "cell_type": "code", "source": "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n               privileged_groups=privileged_groups)\n\n# compute the weights for reweighing the dataset\nRW.fit(dataset_german_train)\n\n# transform the dataset to a new dataset based on the estimated transformation\ndataset_rw_train = RW.transform(dataset_german_train)\ndataset_rw_test = RW.transform(dataset_german_test)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(\"Metrics for Original Training Dataset:\")\nprint(\"\\tDifference in mean outcomes between unprivileged and privileged groups = %f\" % metric_german_train.mean_difference())\nprint(\"\\tdisparate_impact = %f\" % metric_german_train.disparate_impact())\n#print(\"\\tconsistency = %f\" % metric_german_train.consistency())\n#print(\"\\tbase_rate = %f\" % metric_german_train.base_rate())\n#print(\"\\tnum_negatives = %f\" % metric_german_train.num_negatives())\n#print(\"\\tnum_positives = %f\" % metric_german_train.num_positives())\n#print(\"\\tsmoothed_empirical_differential_fairness = %f\" % metric_german_train.smoothed_empirical_differential_fairness())\n\nmetric_rw_train = BinaryLabelDatasetMetric(dataset_rw_train, \n                                         unprivileged_groups=unprivileged_groups,\n                                         privileged_groups=privileged_groups)\n\nprint(\"Metrics for Reweighted Training Dataset:\")\nprint(\"\\tDifference in mean outcomes between unprivileged and privileged groups = %f\" % metric_rw_train.mean_difference())\nprint(\"\\tdisparate_impact = %f\" % metric_rw_train.disparate_impact())\n#print(\"\\tconsistency = %f\" % metric_rw_train.consistency())\n#print(\"\\tbase_rate = %f\" % metric_rw_train.base_rate())\n#print(\"\\tnum_negatives = %f\" % metric_rw_train.num_negatives())\n#print(\"\\tnum_positives = %f\" % metric_rw_train.num_positives())\n#print(\"\\tsmoothed_empirical_differential_fairness = %f\" % metric_rw_train.smoothed_empirical_differential_fairness())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Build Model Using Reweighted Training Data\n\nWe can now train a new machine learning model using the reweighted training data. For this exercise, we will simply use Logistic Regression and then use the model to make predicitons on the test dataset."}, {"metadata": {}, "cell_type": "code", "source": "# scale data\nscale_rw = StandardScaler().fit(dataset_rw_train.features)\n\nX_train_rw = scale_rw.transform(dataset_rw_train.features)\ny_train_rw = dataset_rw_train.labels.ravel()\nw_train_rw = dataset_rw_train.instance_weights.ravel()\n\nX_test_rw = scale_rw.transform(dataset_rw_test.features)\ny_test_rw = dataset_rw_test.labels.ravel()\nw_test_rw = dataset_rw_test.instance_weights.ravel()\n\n#dataset_rw_train.instance_weights", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# create a new instance of the model\nlmod_rw = LogisticRegression()\n\n# train the model\nlmod_rw.fit(X_train_rw, y_train_rw, \n         sample_weight=dataset_rw_train.instance_weights)\n\n# calculate predicted labels\ny_train_pred_rw = lmod_rw.predict(X_train_rw)\n\n# assign positive class index\npos_ind_rw = np.where(lmod_rw.classes_ == dataset_rw_train.favorable_label)[0][0]\n\n# add predicted labels to predictions dataset\ndataset_rw_train_pred = dataset_rw_train.copy()\ndataset_rw_train_pred.labels = y_train_pred_rw\n\ndataset_rw_test_pred = dataset_rw_test.copy()\nX_test = scale_rw.transform(dataset_rw_test_pred.features)\ny_test_pred = lmod.predict(X_test)\npos_ind_test = np.where(lmod_rw.classes_ == dataset_rw_test.favorable_label)[0][0]\ndataset_rw_test_pred.labels = lmod_rw.predict(X_test)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# model accuracy\nprint(\"Original Logistic Regression model accuracy = %f\" % score)\nscore_rw = lmod_rw.score(X_test_rw, y_test_rw)\nprint(\"Reweighted Logistic Regression model accuracy = %f\" % score_rw)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Metrics based on the `ClassificationMetric class`\n\nSince we have built a classification model, we can explore the [`ClassificationMetric class metrics`](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html) for bias in predictions"}, {"metadata": {}, "cell_type": "code", "source": "metric_german_train_pred_rw = ClassificationMetric(dataset_rw_train, dataset_rw_train_pred,\n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\n\nmetric_german_pred_rw = ClassificationMetric(dataset_rw_test, dataset_rw_test_pred,\n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(\"Original test dataset:\")\nprint(\"\\tmean_difference = %f\" % metric_german_test.mean_difference())\nprint(\"\\tdisparate_impact = %f\" % metric_german_test.disparate_impact())\n\nprint(\"Predicted labels (test dataset):\")\nprint(\"\\tmean_difference = %f\" % metric_german_pred.mean_difference())\nprint(\"\\tdisparate_impact = %f\" % metric_german_pred.disparate_impact())\n\nprint(\"Predicted labels (reweighted test dataset):\")\nprint(\"\\tmean_difference = %f\" % metric_german_pred_rw.mean_difference())\nprint(\"\\tdisparate_impact = %f\" % metric_german_pred_rw.disparate_impact())\n\nprint(\"Quality metrics  (test dataset):\")\nprint(\"\\taccuracy = %f\" % metric_german_pred.accuracy())\nprint(\"\\tcoefficient_of_variation = %f\" % metric_german_pred.coefficient_of_variation())\nprint(\"\\terror_rate = %f\" % metric_german_pred.error_rate())\n\nprint(\"Quality metrics  (reweighted test dataset):\")\nprint(\"\\taccuracy = %f\" % metric_german_pred_rw.accuracy())\nprint(\"\\tcoefficient_of_variation = %f\" % metric_german_pred_rw.coefficient_of_variation())\nprint(\"\\terror_rate = %f\" % metric_german_pred_rw.error_rate())\n\n", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "[fig, ax] = plt.subplots(2,2, figsize=(15, 12));\nplot_confusion_matrix(lmod, X_test[df_test['age']==0], y_test[df_test['age']==0],\n                      cmap=plt.cm.Blues, \n                      display_labels=['good credit','bad credit'],\n                      ax=ax[0,0]);\nax[0,0].set_title('Age < 25')\n\nplot_confusion_matrix(lmod, X_test[df_test['age']==1], y_test[df_test['age']==1],\n                      cmap=plt.cm.Blues, \n                      display_labels=['good credit','bad credit'],\n                      ax=ax[0,1]);\nax[0,1].set_title('Age > 25');\n\nplot_confusion_matrix(lmod_rw, X_test_rw[df_test['age']==0], y_test_rw[df_test['age']==0],\n                      cmap=plt.cm.Blues, \n                      display_labels=['good credit','bad credit'],\n                      ax=ax[1,0]);\nax[1,0].set_title('Age < 25 (Reweighted)')\n\nplot_confusion_matrix(lmod_rw, X_test_rw[df_test['age']==1], y_test_rw[df_test['age']==1],\n                      cmap=plt.cm.Blues, \n                      display_labels=['good credit','bad credit'],\n                      ax=ax[1,1]);\nax[1,1].set_title('Age > 25 (Reweighted)');", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Other debiasing Algorithms\n\nThere are too many algorithms and points in the process we can look to mitigate bias to explore in this lab. Some other debiasing algorithsm include:\n\n- Another example of pre-processing debiasing is the optimized data pre-processing algorithm. The debiasing function used is implemented in the [OptimPreproc](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.OptimPreproc.html?highlight=get%20distortion) class. It modifies training data features & labels. See the example notebook here: https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_optim_data_preproc.ipynb\n\n- There are several in-processing algorithms. Including:\n  - [Adversarial Debiasing](https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_adversarial_debiasing.ipynb) - Uses adversarial techniques to maximize accuracy & reduce evidence of protected attributes in predictions\n  - [Reject Option Classification](https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_reject_option_classification.ipynb) - Adds a discrimination-aware regularization term to the learning objective\n  \n- There are several post-processing algorithms. Including:\n  - [Reject option classification (ROC)](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html?highlight=reject) is a postprocessing technique that gives favorable outcomes to unpriviliged groups and unfavorable outcomes to priviliged groups in a confidence band around the decision boundary with the highest uncertainty.\n\nAnother full example: https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/tutorial_medical_expenditure.ipynb"}, {"metadata": {}, "cell_type": "markdown", "source": "## Acknowledgements and Credits\n\nThis notebook is adapted from several examples & tutorials:\n\n- https://github.com/IBMDeveloperUK/Trusted-AI-Workshops\n- https://github.com/Trusted-AI/AIF360/tree/master/examples\n- https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/tutorial_credit_scoring.ipynb\n    \nCopyright \u00a9 2021 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}