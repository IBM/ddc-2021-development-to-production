{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Analyzing Credit Risk with Cloud Pak for Data \u00b6 Welcome to our workshop! In this workshop we'll be using the Cloud Pak for Data platform to Collect Data, Organize Data, Analyze Data, and Infuse AI into our applications. About this workshop \u00b6 In this workshop we will be using a credit risk / lending scenario. In this scenario, lenders respond to an increased pressure to expand lending to larger and more diverse audiences, by using different approaches to risk modeling. This means going beyond traditional credit data sources to alternative credit sources (i.e. mobile phone plan payment histories, education, etc), which may introduce risk of bias or other unexpected correlations. The credit risk model that we are exploring in this workshop uses a training data set that contains 20 attributes about each loan applicant. The scenario and model use synthetic data based on the UCI German Credit dataset . The data is split into three CSV files and are located in the data directory of the GitHub repository. Agenda \u00b6 Topic Description Type Introduction Course/Workshop Introduction Lecture Platform Overview Cloud Pak for Data overview Lecture Environment Setup Environment provisioning and setup Hands-on lab Data Wrangling Data wrangling overview Lecture Data Wrangling using Data Refinery Data aggregation, processing and wrangling Hands-on lab Machine Learning Machine Learning overview Lecture Automated ML with AutoAI Build and save predictive models using AutoAI Hands-on lab Model Deployment Model Deployment overview Lecture Online Deployment & Testing Deploy a model for real time predictions Hands-on lab Batch Deployment & Testing Deploy a model for batch procesing Hands-on lab Model Integration to Python Application Invoke model endpoint from an application Hands-on lab Trusted AI Trusted AI Overview Lecture Trust and Transparency Using the AIF360 and AIX360 toolkits Hands-on lab Model Versioning Updating Models Overview Lecture Versioning Models and Deployments Update ML models Hands-on lab Prescriptive Models Decision Optimization (DO) Overview Lecture Building and Deploying a DO Model Use CPLEX to build a model and deploy to WML Hands-on lab Conclusion Wrap up discussion Lecture Compatability \u00b6 This workshop has been tested on the following platforms: macOS : Mojave (10.14), Catalina (10.15) Google Chrome version 81","title":"About the workshop"},{"location":"#analyzing-credit-risk-with-cloud-pak-for-data","text":"Welcome to our workshop! In this workshop we'll be using the Cloud Pak for Data platform to Collect Data, Organize Data, Analyze Data, and Infuse AI into our applications.","title":"Analyzing Credit Risk with Cloud Pak for Data"},{"location":"#about-this-workshop","text":"In this workshop we will be using a credit risk / lending scenario. In this scenario, lenders respond to an increased pressure to expand lending to larger and more diverse audiences, by using different approaches to risk modeling. This means going beyond traditional credit data sources to alternative credit sources (i.e. mobile phone plan payment histories, education, etc), which may introduce risk of bias or other unexpected correlations. The credit risk model that we are exploring in this workshop uses a training data set that contains 20 attributes about each loan applicant. The scenario and model use synthetic data based on the UCI German Credit dataset . The data is split into three CSV files and are located in the data directory of the GitHub repository.","title":"About this workshop"},{"location":"#agenda","text":"Topic Description Type Introduction Course/Workshop Introduction Lecture Platform Overview Cloud Pak for Data overview Lecture Environment Setup Environment provisioning and setup Hands-on lab Data Wrangling Data wrangling overview Lecture Data Wrangling using Data Refinery Data aggregation, processing and wrangling Hands-on lab Machine Learning Machine Learning overview Lecture Automated ML with AutoAI Build and save predictive models using AutoAI Hands-on lab Model Deployment Model Deployment overview Lecture Online Deployment & Testing Deploy a model for real time predictions Hands-on lab Batch Deployment & Testing Deploy a model for batch procesing Hands-on lab Model Integration to Python Application Invoke model endpoint from an application Hands-on lab Trusted AI Trusted AI Overview Lecture Trust and Transparency Using the AIF360 and AIX360 toolkits Hands-on lab Model Versioning Updating Models Overview Lecture Versioning Models and Deployments Update ML models Hands-on lab Prescriptive Models Decision Optimization (DO) Overview Lecture Building and Deploying a DO Model Use CPLEX to build a model and deploy to WML Hands-on lab Conclusion Wrap up discussion Lecture","title":"Agenda"},{"location":"#compatability","text":"This workshop has been tested on the following platforms: macOS : Mojave (10.14), Catalina (10.15) Google Chrome version 81","title":"Compatability"},{"location":"data-processing-refinery/","text":"Data Processing with Data Refinery \u00b6 In this module, we will prepare our data assets for analysis. We will use the Data Refinery graphical flow editor tool to create a set of ordered operations that will cleanse and shape our data. We will also explore the graphical interface to profile data and create visualizations to get a perspective and insights into the dataset. This section is broken up into the following steps: Merge and Cleanse Data Profile Data Visualize Data Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. Merge and Cleanse Data \u00b6 We will start by wrangling, shaping and refining our data. To do this, we will create a refinery flow to contain a series of data transformation steps. Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. To create a data refinery flow, click the Add to project button from the top of the page and click the Data Refinery flow option. Select Data assets on the left panel, then select the application_loan_data.csv data asset. Then click the Add button. The first thing we want to do is create a merged dataset. Start by joining the loan data with information about the loan application. Click the Operation button on the top left and then scroll down and select the Join operation. From the drop down list, select Inner join and then click the Add data set link to select the data asset you are going to join with. Select Data assets on the left panel and this time select the application_personal_data.csv data asset. Then click the Apply button. Finish setting the following values and then click the Next button: Under the Source *Suffix option, enter _loan_ds . Under the Data set to join *Suffix option, enter _personal_ds . Under the Join keys, click the input box and select CustomerID for both data sets. Although we could modify what columns will be in the joined dataset, we will leave the default and include them all. Click the Apply button. Repeat the five previous steps to join our other dataset (i.e the financial information for this applicant). Click the Operation button on the top left and then scroll down and select the Join operation. Set the following values and then click the Next button: Inner join Data set to join: application_financial_data.csv Under the Source *Suffix option, enter _loan_ds . Under the Data set to join *Suffix option, enter _financial_ds . Under the Join keys, click the input box and select CustomerID for both data sets. Click the Apply button to finish this final join. Let's say we've decide that there are columns that we don't want to leave in our dataset ( maybe because they might not be useful features in our Machine Learning model, or because we don't want to make those data attributes accessible to others, or any other reason). We'll remove the FirstName , LastName , Email , StreetAddress , City , State , PostalCode columns. For each column to be removed: Click the Operation + button, then select the Remove operation. In the Select column drop down, choose one of the columns to remove (i.e FirstName ). Click the Next button and then the Apply button. The column will be removed. Repeat the above two steps to remove the remaining six columns. Finally, we want to ensure there is no duplicates in our dataset. Click the Operation button once again and click the Remove duplicates operation. Select CustomerID as the column and click the Next button. Then click the Apply button in the subsequent panel. At this point, you have a data transformation flow with 11 steps. The flow keeps track of each of the steps and we can even undo (or redo) an action using the circular arrows. To see the steps in the data flow that you have performed, click the Steps button. The operations that you have performed on the data will be shown. You can modify these steps and/or save for future use. Lets edit the flow name and output options. Click on the Information icon on the top right and then click the Edit button. Click the pencil icon next to Data Refinery Flow Name , set the name to credit_risk_wrangling_cleaning_flow and click the Apply button. Then click the Edit Output pencil icon and set the name to credit_risk_shaped.csv (leave the rest of the CSV output defaults) and click the 'Check mark icon'. Finally, click the Done button Click the Save icon to save the flow. Run Data Flow Job \u00b6 Data Refinery allows you to run these data flow jobs on demand or at scheduled times. In this way, you can regularly refine new data as it is updated. Click on the Jobs icon and then Save and create a job option from the menu. Give the job a name and optional description. Click the Next button. Click Next on the next two screens, leaving the default selections. You will reach the Review and create screen. Note the output name, which is credit_risk_shaped . Click the Create and run button. When the job is successfully created, you will receive a notification. Click on the job details link in the notification panel to see the job status. The job will be listed with a status of Running and then the status will change to Completed . Once its completed, click the Edit configuration button. Click the pencil icon next to Schedule . Notice that you can toggle the Schedule to run switch and choose a date and time to run this transformation as a job. We will not run this as a job, so go ahead and click the Cancel link. Profile Data \u00b6 Go back to the project by clicking the name of the project in the breadcrumbs in the top left area of the browser. Click the Assets tab and then scroll down to the Data Refinery flows section and click on the credit_risk_wrangling_cleaning_flow flow. Wait for the flow operations to be applied and then click on the Profile tab will bring up a view of several statistics and histograms for the attributes in your data. You can get insight into the data from the views and statistics: The median age of the applicants is 36, with the bulk under 49. About as many people had credits_paid_to_date as prior_payments_delayed. Few had no_credits. Over three times more loan applicants have no checking than those with greater than 200 in checking. Visualize Data \u00b6 Let's do some visual exploration of our data using charts and graphs. Note that this is an exploratory phase and we're looking for insights in out data. We can accomplish this in Data Refinery interactively without coding. Choose the Visualizations tab to bring up the page where you can select columns that you want to visualize. Add LoanAmount as the first column and click Add Column to add another column. Next add LoanDuration and click Visualize . The system will pick a suggested plot for you based on your data and show more suggested plot types at the top. Remember that we are most interested in knowing how these features impact a loan being at the risk. So, let's add the Risk as a color on top of our current scatter plot. That should help us visually see if there's something of interest here. From the left, click the Color Map section and select Risk . Also, to see the full data, drag the right side of the data selector at the bottom all the way to the right, in order to show all the data inside your plot. We notice that there are more risk (purple in this chart) on this plot towards the top right, than there is on the bottom left. This is a good start as it shows that there is probably a relationship between the riskiness of a loan and its duration and amount. It appears that the higher the amount and duration, the riskier the loan. Interesting, let's dig in further in how the loan duration could play into the riskiness of a loan. Let's plot a histogram of the LoanDuration to see if we can notice anything. First, select Histogram from the Chart Type . Next on the left, select Risk in the Split By section, select the Stacked radio button, and uncheck the Show kde curve , as well as the Show distribution curve options. You should see a chart that looks like the following image (move the bin width down to 1 if necessary). It looks like the longer the duration the larger the blue bar (risky loan count) become and the smaller the purple bars (non risky loan count) become. That indicate loans with longer duration are in general more likely to be risky. However, we need more information. We next explore if there is some insight in terms of the riskiness of a loan based on its duration when broken down by the loan purpose. To do so, let's create a Heat Map plot. At the top of the page, in the Chart Type section, open the arrows on the right, select Heat Map (accept the warning if prompted). Next, select Risk in the column section and LoanPurpose for the Row section. Additionally, to see the effects of the loan duration, select Mean in the summary section, and select LoanDuration in the Value section. You can now see that the least risky loans are those taken out for purchasing a new car and they are on average 10 years long. To the left of that cell we see that loans taken out for the same purpose that average around 15 years for term length seem to be more risky. So one could conclude the longer the loan term is, the more likely it will be risky. In contrast, we can see that both risky and non-risky loans for the other category seem to have the same average term length, so one could conclude that there's little, if any, relationship between loan length and its riskiness for the loans of type other . In general, for each row, the bigger the color difference between the right and left column, the more likely that loan duration plays a role for the riskiness of the loan category. Now let's look into customizing our plot. Under the Actions panel, notice that you can perform tasks such as Start over , Download chart details , Download chart image , or set Global visualization preferences ( Note: Hover over the icons to see the names ). Click on the gear icon in the Actions panel. We see that we can do things in the Global visualization preferences for Titles , Tools , Theme , and Notifications . Click on the Theme tab and update the color scheme to Dark . Then click the Apply button, now the colors for all of our charts will reflect this. Play around with various Themes and find one that you like. Finally, to save our plot as an image, click on the image icon on the top right, highlighted below, and then save the image. Conclusion \u00b6 We've seen a some of the capabilities of the Data Refinery. We saw how we can transform data using R code, as well as using various operations on the columns such as changing the data type, removing empty rows, or deleting the column altogether. We next saw that all the steps in our Data Flow are recorded, so we can remove steps, repeat them, or edit an individual step. We were able to quickly profile the data, to see histograms and statistics for each column. And finally we created more in-depth Visualizations, creating a scatter plot, histogram, and heatmap to explore the relationship between the riskiness of a loan and its duration, and purpose.","title":"Data Wrangling with Data Refinery"},{"location":"data-processing-refinery/#data-processing-with-data-refinery","text":"In this module, we will prepare our data assets for analysis. We will use the Data Refinery graphical flow editor tool to create a set of ordered operations that will cleanse and shape our data. We will also explore the graphical interface to profile data and create visualizations to get a perspective and insights into the dataset. This section is broken up into the following steps: Merge and Cleanse Data Profile Data Visualize Data Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space.","title":"Data Processing with Data Refinery"},{"location":"data-processing-refinery/#merge-and-cleanse-data","text":"We will start by wrangling, shaping and refining our data. To do this, we will create a refinery flow to contain a series of data transformation steps. Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. To create a data refinery flow, click the Add to project button from the top of the page and click the Data Refinery flow option. Select Data assets on the left panel, then select the application_loan_data.csv data asset. Then click the Add button. The first thing we want to do is create a merged dataset. Start by joining the loan data with information about the loan application. Click the Operation button on the top left and then scroll down and select the Join operation. From the drop down list, select Inner join and then click the Add data set link to select the data asset you are going to join with. Select Data assets on the left panel and this time select the application_personal_data.csv data asset. Then click the Apply button. Finish setting the following values and then click the Next button: Under the Source *Suffix option, enter _loan_ds . Under the Data set to join *Suffix option, enter _personal_ds . Under the Join keys, click the input box and select CustomerID for both data sets. Although we could modify what columns will be in the joined dataset, we will leave the default and include them all. Click the Apply button. Repeat the five previous steps to join our other dataset (i.e the financial information for this applicant). Click the Operation button on the top left and then scroll down and select the Join operation. Set the following values and then click the Next button: Inner join Data set to join: application_financial_data.csv Under the Source *Suffix option, enter _loan_ds . Under the Data set to join *Suffix option, enter _financial_ds . Under the Join keys, click the input box and select CustomerID for both data sets. Click the Apply button to finish this final join. Let's say we've decide that there are columns that we don't want to leave in our dataset ( maybe because they might not be useful features in our Machine Learning model, or because we don't want to make those data attributes accessible to others, or any other reason). We'll remove the FirstName , LastName , Email , StreetAddress , City , State , PostalCode columns. For each column to be removed: Click the Operation + button, then select the Remove operation. In the Select column drop down, choose one of the columns to remove (i.e FirstName ). Click the Next button and then the Apply button. The column will be removed. Repeat the above two steps to remove the remaining six columns. Finally, we want to ensure there is no duplicates in our dataset. Click the Operation button once again and click the Remove duplicates operation. Select CustomerID as the column and click the Next button. Then click the Apply button in the subsequent panel. At this point, you have a data transformation flow with 11 steps. The flow keeps track of each of the steps and we can even undo (or redo) an action using the circular arrows. To see the steps in the data flow that you have performed, click the Steps button. The operations that you have performed on the data will be shown. You can modify these steps and/or save for future use. Lets edit the flow name and output options. Click on the Information icon on the top right and then click the Edit button. Click the pencil icon next to Data Refinery Flow Name , set the name to credit_risk_wrangling_cleaning_flow and click the Apply button. Then click the Edit Output pencil icon and set the name to credit_risk_shaped.csv (leave the rest of the CSV output defaults) and click the 'Check mark icon'. Finally, click the Done button Click the Save icon to save the flow.","title":"Merge and Cleanse Data"},{"location":"data-processing-refinery/#run-data-flow-job","text":"Data Refinery allows you to run these data flow jobs on demand or at scheduled times. In this way, you can regularly refine new data as it is updated. Click on the Jobs icon and then Save and create a job option from the menu. Give the job a name and optional description. Click the Next button. Click Next on the next two screens, leaving the default selections. You will reach the Review and create screen. Note the output name, which is credit_risk_shaped . Click the Create and run button. When the job is successfully created, you will receive a notification. Click on the job details link in the notification panel to see the job status. The job will be listed with a status of Running and then the status will change to Completed . Once its completed, click the Edit configuration button. Click the pencil icon next to Schedule . Notice that you can toggle the Schedule to run switch and choose a date and time to run this transformation as a job. We will not run this as a job, so go ahead and click the Cancel link.","title":"Run Data Flow Job"},{"location":"data-processing-refinery/#profile-data","text":"Go back to the project by clicking the name of the project in the breadcrumbs in the top left area of the browser. Click the Assets tab and then scroll down to the Data Refinery flows section and click on the credit_risk_wrangling_cleaning_flow flow. Wait for the flow operations to be applied and then click on the Profile tab will bring up a view of several statistics and histograms for the attributes in your data. You can get insight into the data from the views and statistics: The median age of the applicants is 36, with the bulk under 49. About as many people had credits_paid_to_date as prior_payments_delayed. Few had no_credits. Over three times more loan applicants have no checking than those with greater than 200 in checking.","title":"Profile Data"},{"location":"data-processing-refinery/#visualize-data","text":"Let's do some visual exploration of our data using charts and graphs. Note that this is an exploratory phase and we're looking for insights in out data. We can accomplish this in Data Refinery interactively without coding. Choose the Visualizations tab to bring up the page where you can select columns that you want to visualize. Add LoanAmount as the first column and click Add Column to add another column. Next add LoanDuration and click Visualize . The system will pick a suggested plot for you based on your data and show more suggested plot types at the top. Remember that we are most interested in knowing how these features impact a loan being at the risk. So, let's add the Risk as a color on top of our current scatter plot. That should help us visually see if there's something of interest here. From the left, click the Color Map section and select Risk . Also, to see the full data, drag the right side of the data selector at the bottom all the way to the right, in order to show all the data inside your plot. We notice that there are more risk (purple in this chart) on this plot towards the top right, than there is on the bottom left. This is a good start as it shows that there is probably a relationship between the riskiness of a loan and its duration and amount. It appears that the higher the amount and duration, the riskier the loan. Interesting, let's dig in further in how the loan duration could play into the riskiness of a loan. Let's plot a histogram of the LoanDuration to see if we can notice anything. First, select Histogram from the Chart Type . Next on the left, select Risk in the Split By section, select the Stacked radio button, and uncheck the Show kde curve , as well as the Show distribution curve options. You should see a chart that looks like the following image (move the bin width down to 1 if necessary). It looks like the longer the duration the larger the blue bar (risky loan count) become and the smaller the purple bars (non risky loan count) become. That indicate loans with longer duration are in general more likely to be risky. However, we need more information. We next explore if there is some insight in terms of the riskiness of a loan based on its duration when broken down by the loan purpose. To do so, let's create a Heat Map plot. At the top of the page, in the Chart Type section, open the arrows on the right, select Heat Map (accept the warning if prompted). Next, select Risk in the column section and LoanPurpose for the Row section. Additionally, to see the effects of the loan duration, select Mean in the summary section, and select LoanDuration in the Value section. You can now see that the least risky loans are those taken out for purchasing a new car and they are on average 10 years long. To the left of that cell we see that loans taken out for the same purpose that average around 15 years for term length seem to be more risky. So one could conclude the longer the loan term is, the more likely it will be risky. In contrast, we can see that both risky and non-risky loans for the other category seem to have the same average term length, so one could conclude that there's little, if any, relationship between loan length and its riskiness for the loans of type other . In general, for each row, the bigger the color difference between the right and left column, the more likely that loan duration plays a role for the riskiness of the loan category. Now let's look into customizing our plot. Under the Actions panel, notice that you can perform tasks such as Start over , Download chart details , Download chart image , or set Global visualization preferences ( Note: Hover over the icons to see the names ). Click on the gear icon in the Actions panel. We see that we can do things in the Global visualization preferences for Titles , Tools , Theme , and Notifications . Click on the Theme tab and update the color scheme to Dark . Then click the Apply button, now the colors for all of our charts will reflect this. Play around with various Themes and find one that you like. Finally, to save our plot as an image, click on the image icon on the top right, highlighted below, and then save the image.","title":"Visualize Data"},{"location":"data-processing-refinery/#conclusion","text":"We've seen a some of the capabilities of the Data Refinery. We saw how we can transform data using R code, as well as using various operations on the columns such as changing the data type, removing empty rows, or deleting the column altogether. We next saw that all the steps in our Data Flow are recorded, so we can remove steps, repeat them, or edit an individual step. We were able to quickly profile the data, to see histograms and statistics for each column. And finally we created more in-depth Visualizations, creating a scatter plot, histogram, and heatmap to explore the relationship between the riskiness of a loan and its duration, and purpose.","title":"Conclusion"},{"location":"faq/","text":"Frequently Asked Questions & Helpful Tips / Tricks \u00b6 Account and Service Creation \u00b6 Q: I don't have all the services needed Q: I get a That email address is already registered to an IBM Cloud account. messsage Q: I get a Your Watson Studio, Watson Knowledge Catalog, and Watson Machine Learning Lite services must be created in the same service region. error AutoAI Labs \u00b6 Q: I am running the AutoAI generated notebooks but see package version errors. Account Sign up and Service Creation \u00b6 Q: I don't have all the services needed \u00b6 A: In some rare cases, the services will not automatically provision for you. You can do that manually by following these instructions: Once you are on IBM Cloud Pak for Data, on the top right corner click on your avatar, and then click on Profile and settings . Go to the Services tab. If the Machine Learning service instance is not listed under You Cloud Pak for Data Services then find it in the Try our Available Services section and click on the Add button. Next, note down the name of the Machine Learning service instance in your Cloud Pak for Data section. This is the blue hyperlink underneeth the Machine Learning card title. You will need to provide this name in future steps. Q: I get the That email address is already registered to an IBM Cloud account. messsage \u00b6 A: You must already have an IBMid account. Follow the login link provided in the error message to login to your existing account. Q: I get the Your Watson Studio, Watson Knowledge Catalog, and Watson Machine Learning Lite services must be created in the same service region. error \u00b6 A: This means you have previously created some Watson services in a different region. To resolve this, go to the CP4DaaS Login page, select the region you had previously used and then login using the login link at the bottom right. Alternatively, you can create a new account and proceed as a new user to follow along. AutoAI Labs \u00b6 Q: I am running the AutoAI generated notebooks but see package version errors \u00b6 A: You need to restart the Jupyter Kernel to ensure the libraries installed in the notebook are found and used.","title":"FAQs / Tips"},{"location":"faq/#frequently-asked-questions-helpful-tips-tricks","text":"","title":"Frequently Asked Questions &amp; Helpful Tips / Tricks"},{"location":"faq/#account-and-service-creation","text":"Q: I don't have all the services needed Q: I get a That email address is already registered to an IBM Cloud account. messsage Q: I get a Your Watson Studio, Watson Knowledge Catalog, and Watson Machine Learning Lite services must be created in the same service region. error","title":"Account and Service Creation"},{"location":"faq/#autoai-labs","text":"Q: I am running the AutoAI generated notebooks but see package version errors.","title":"AutoAI Labs"},{"location":"faq/#account-sign-up-and-service-creation","text":"","title":"Account Sign up and Service Creation"},{"location":"faq/#q-i-dont-have-all-the-services-needed","text":"A: In some rare cases, the services will not automatically provision for you. You can do that manually by following these instructions: Once you are on IBM Cloud Pak for Data, on the top right corner click on your avatar, and then click on Profile and settings . Go to the Services tab. If the Machine Learning service instance is not listed under You Cloud Pak for Data Services then find it in the Try our Available Services section and click on the Add button. Next, note down the name of the Machine Learning service instance in your Cloud Pak for Data section. This is the blue hyperlink underneeth the Machine Learning card title. You will need to provide this name in future steps.","title":"Q: I don't have all the services needed"},{"location":"faq/#q-i-get-the-that-email-address-is-already-registered-to-an-ibm-cloud-account-messsage","text":"A: You must already have an IBMid account. Follow the login link provided in the error message to login to your existing account.","title":"Q: I get the That email address is already registered to an IBM Cloud account. messsage"},{"location":"faq/#q-i-get-the-your-watson-studio-watson-knowledge-catalog-and-watson-machine-learning-lite-services-must-be-created-in-the-same-service-region-error","text":"A: This means you have previously created some Watson services in a different region. To resolve this, go to the CP4DaaS Login page, select the region you had previously used and then login using the login link at the bottom right. Alternatively, you can create a new account and proceed as a new user to follow along.","title":"Q: I get the Your Watson Studio, Watson Knowledge Catalog, and Watson Machine Learning Lite services must be created in the same service region. error"},{"location":"faq/#autoai-labs_1","text":"","title":"AutoAI Labs"},{"location":"faq/#q-i-am-running-the-autoai-generated-notebooks-but-see-package-version-errors","text":"A: You need to restart the Jupyter Kernel to ensure the libraries installed in the notebook are found and used.","title":"Q: I am running the AutoAI generated notebooks but see package version errors"},{"location":"ml-model-deployment/","text":"Machine Learning Model Deployments, Inferencing and Integration \u00b6 There are three labs in this module: Online Deployment - Create a REST endpoint for your model that you can use for near real time inferencing (predictions) Batch Deployment - Create an endpoint to submit groups of payload data (either inline or as data assets) using jobs to your model for predictions. Application Integration of Online Deployment - Integrate an online deployment endpoint to a sample end user application.","title":"Machine Learning Model Deployments, Inferencing and Integration"},{"location":"ml-model-deployment/#machine-learning-model-deployments-inferencing-and-integration","text":"There are three labs in this module: Online Deployment - Create a REST endpoint for your model that you can use for near real time inferencing (predictions) Batch Deployment - Create an endpoint to submit groups of payload data (either inline or as data assets) using jobs to your model for predictions. Application Integration of Online Deployment - Integrate an online deployment endpoint to a sample end user application.","title":"Machine Learning Model Deployments, Inferencing and Integration"},{"location":"ml-model-deployment/batch-model-deployment/","text":"Machine Learning Model Batch Deployment and Scoring \u00b6 In this module, we will learn how to deploy our Machine Learning models. By doing so, we make them available for use in production such that applications and business processes can derive insights from them. There are several types of deployments available ( depending on the model framework used ). In this lab, we will explore: Batch Deployments - Allows you to run the model against data as a batch process. This lab will build a batch deployment and test the model endpoint by submitting a job to score a batch set of inputs. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. If you would have any issues, check the FAQ section . Creating Batch Model Deployment \u00b6 Another approach to expose the model to be consumed by other users/applications is to create a batch deployment. This type of deployment will make an instance of the model available to make predictions against data assets or groups of records. The model prediction requests are scheduled as jobs, which are executed asynchronously. Lets start by creating the deployment: Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, click on the Assets tab and in the 'Model' table, find the model name for the model you previously built and now want to create a deployment against. Use your mouse to hover over the right side of that table row and click the Deploy rocket icon (the icons are not visible by default until you hover over them). Note: There may be more than one model listed in them 'Models' section. This can happen if you have run the Jupyter notebook that creates SparkML models more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. On the 'Create a deployment' screen: choose Batch for the Deployment Type , give the deployment a name and optional description. From the 'Hardware definition' drop down, select the smallest option ( 2 CPU, 8GB RAM in this case though for large or frequent batch jobs, you might choose to scale the hardware up depending on what is available to your account/plan). Click the Create button. Once the status shows as Deployed you will be able to start submitting jobs to the deployment. Create and Schedule a Job \u00b6 Next we can schedule a job to run against our batch deployment. We could create a job, with specific input data (or data asset) and schedule, either programmatically or through the UI (i.e the \"Create Job\" button on the deployment page). For this lab, we are going to do this programmatically using the Python client SDK. For this part of the exercise we're going to use a Jupyter notebook to create and submit a batch job to our model deployment. Note: The batch job input is impacted by the machine learning framework used to build the model. Currently, SparkML based model batch jobs require inline payload to be used. For other frameworks, we can use data assets (i.e CSV files) as the input payload. Run the Batch Notebook \u00b6 The Jupyter notebook is already included as an asset in the project you imported during the setup module. Go the (\u2630) navigation menu and click on the Projects link and then click on the project you created during the setup. From the project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the machinelearning-creditrisk-batchscoring notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. Notebook sections \u00b6 With the notebook open, spend a minute looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Please note that some of the comments in the notebook are directions for you to modify specific sections of the code. These are written in red . Perform any changes necessary, as indicated in the cells, before executing them. Execute the cells one at a time. Section 1.0 Install required packages will install some of the libraries we are going to use in the notebook. Note that we upgrade the installed version of Watson Machine Learning Python Client. Ensure the output of the first code cell is that the python packages were successfully installed. Section 2.0 Create Batch Deployment Job will create a job for the batch deployment. To do that, we will use the Watson Machine Learning client to get our deployment and create a job. In the first code cell for Section2.1 , you will need to provide your IBM Cloud API key and Machine learning service location, which you should have from the setup module. In section 2.2, be sure to update the DEPLOYMENT_SPACE_NAME variable with your deployment space name (copy and past the name which is within the output of the previous code cell). In section 2.3, be sure to update the DEPLOYMENT_NAME variable with the name of the batch deployment you created previously (copy and past the name which is within the output of the previous code cell). Continue to run the rest of the cells in section 2 which will load the batch input data set and create the job. The last code cell in section 2 will show that the job is in a queued state. Section 3.0 Monitor Batch Job Status will start polling the job status until it completes or fails. The code cell will output the status every 5 seconds as the job goes from queued to running to completed or failed. Once the job completes, continue to run the cells until the end of the notebook. Stop the Environment \u00b6 In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Important: If you have completed this section and do not plan on completing the other optional deployment approaches, please go ahead and cleanup your deployment. Follow the Cleanup Deployment instructions. Conclusion \u00b6 Congratulations. You've completed this lab and seen how to create and test batch deployments for your machine learning models.","title":"Creating and Testing a Batch Endpoint"},{"location":"ml-model-deployment/batch-model-deployment/#machine-learning-model-batch-deployment-and-scoring","text":"In this module, we will learn how to deploy our Machine Learning models. By doing so, we make them available for use in production such that applications and business processes can derive insights from them. There are several types of deployments available ( depending on the model framework used ). In this lab, we will explore: Batch Deployments - Allows you to run the model against data as a batch process. This lab will build a batch deployment and test the model endpoint by submitting a job to score a batch set of inputs. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. If you would have any issues, check the FAQ section .","title":"Machine Learning Model Batch Deployment and Scoring"},{"location":"ml-model-deployment/batch-model-deployment/#creating-batch-model-deployment","text":"Another approach to expose the model to be consumed by other users/applications is to create a batch deployment. This type of deployment will make an instance of the model available to make predictions against data assets or groups of records. The model prediction requests are scheduled as jobs, which are executed asynchronously. Lets start by creating the deployment: Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, click on the Assets tab and in the 'Model' table, find the model name for the model you previously built and now want to create a deployment against. Use your mouse to hover over the right side of that table row and click the Deploy rocket icon (the icons are not visible by default until you hover over them). Note: There may be more than one model listed in them 'Models' section. This can happen if you have run the Jupyter notebook that creates SparkML models more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. On the 'Create a deployment' screen: choose Batch for the Deployment Type , give the deployment a name and optional description. From the 'Hardware definition' drop down, select the smallest option ( 2 CPU, 8GB RAM in this case though for large or frequent batch jobs, you might choose to scale the hardware up depending on what is available to your account/plan). Click the Create button. Once the status shows as Deployed you will be able to start submitting jobs to the deployment.","title":"Creating Batch Model Deployment"},{"location":"ml-model-deployment/batch-model-deployment/#create-and-schedule-a-job","text":"Next we can schedule a job to run against our batch deployment. We could create a job, with specific input data (or data asset) and schedule, either programmatically or through the UI (i.e the \"Create Job\" button on the deployment page). For this lab, we are going to do this programmatically using the Python client SDK. For this part of the exercise we're going to use a Jupyter notebook to create and submit a batch job to our model deployment. Note: The batch job input is impacted by the machine learning framework used to build the model. Currently, SparkML based model batch jobs require inline payload to be used. For other frameworks, we can use data assets (i.e CSV files) as the input payload.","title":"Create and Schedule a Job"},{"location":"ml-model-deployment/batch-model-deployment/#run-the-batch-notebook","text":"The Jupyter notebook is already included as an asset in the project you imported during the setup module. Go the (\u2630) navigation menu and click on the Projects link and then click on the project you created during the setup. From the project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the machinelearning-creditrisk-batchscoring notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"Run the Batch Notebook"},{"location":"ml-model-deployment/batch-model-deployment/#notebook-sections","text":"With the notebook open, spend a minute looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Please note that some of the comments in the notebook are directions for you to modify specific sections of the code. These are written in red . Perform any changes necessary, as indicated in the cells, before executing them. Execute the cells one at a time. Section 1.0 Install required packages will install some of the libraries we are going to use in the notebook. Note that we upgrade the installed version of Watson Machine Learning Python Client. Ensure the output of the first code cell is that the python packages were successfully installed. Section 2.0 Create Batch Deployment Job will create a job for the batch deployment. To do that, we will use the Watson Machine Learning client to get our deployment and create a job. In the first code cell for Section2.1 , you will need to provide your IBM Cloud API key and Machine learning service location, which you should have from the setup module. In section 2.2, be sure to update the DEPLOYMENT_SPACE_NAME variable with your deployment space name (copy and past the name which is within the output of the previous code cell). In section 2.3, be sure to update the DEPLOYMENT_NAME variable with the name of the batch deployment you created previously (copy and past the name which is within the output of the previous code cell). Continue to run the rest of the cells in section 2 which will load the batch input data set and create the job. The last code cell in section 2 will show that the job is in a queued state. Section 3.0 Monitor Batch Job Status will start polling the job status until it completes or fails. The code cell will output the status every 5 seconds as the job goes from queued to running to completed or failed. Once the job completes, continue to run the cells until the end of the notebook.","title":"Notebook sections"},{"location":"ml-model-deployment/batch-model-deployment/#stop-the-environment","text":"In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Important: If you have completed this section and do not plan on completing the other optional deployment approaches, please go ahead and cleanup your deployment. Follow the Cleanup Deployment instructions.","title":"Stop the Environment"},{"location":"ml-model-deployment/batch-model-deployment/#conclusion","text":"Congratulations. You've completed this lab and seen how to create and test batch deployments for your machine learning models.","title":"Conclusion"},{"location":"ml-model-deployment/online-model-deployment/","text":"Machine Learning Model Online Deployment and Scoring \u00b6 In this module, we will learn how to deploy our Machine Learning models. By doing so, we make them available for use in production such that applications and business processes can derive insights from them. There are several types of deployments available ( depending on the model framework used ). In this lab, we will explore: Online Deployments - Allows you to run the model on data in real-time, as data is received by a web service. This lab will build an online deployment and test the model endpoint using both the built in testing tool as well as external testing tools. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. If you would have any issues, check the FAQ section . Create Online Model Deployment \u00b6 After a model has been created, saved and promoted to our deployment space, we can proceed to deploying the model. For this section, we will be creating an online deployment. This type of deployment will make an instance of the model available to make predictions in real time via an API. Although we will use the Cloud Pak for Data UI to deploy the model, the same can be done programmatically. Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, click the Assets tab and in the 'Models' table, find the model name for the model you previously built and now want to create a deployment against. Use your mouse to hover over the right side of that table row and click the Deploy rocket icon (the icons are not visible by default until you hover over them). Note: There may be more than one model listed in them 'Models' section. This can happen if you have run the Jupyter notebook that creates SparkML models more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. On the Create a deployment screen, choose Online for the Deployment Type , give the Deployment a name and optionally a description and click the Create button. Click on the Deployments tab. The new deployment status will show as In progress and then switch to Deployed when it is complete. Test Online Model Deployment \u00b6 Cloud Pak for Data offers tools to quickly test out Watson Machine Learning models. We begin with the built-in tooling. From the Model deployment page, once the deployment status shows as Deployed , click on the name of your deployment. The deployment API reference tab shows how to use the model using cURL , Java , Javascript , Python , and Scala . To get to the built-in test tool, click on the Test tab and then click on the Provide input data as JSON icon. Copy and paste the following data objects into the Body panel (replace the text that was in the input panel). Note: Click the tab appropriate for the model you are testing (either an AutoAI model or the SparkML one built using the Jupyter notebook). Also make sure the input below is the only content in the field. Do not append it to the default content { \"input_data\": [.......] } that may already be in the test input panel. AutoAI Model { \"input_data\" : [ { \"fields\" : [ \"CustomerID\" , \"LoanDuration\" , \"LoanPurpose\" , \"LoanAmount\" , \"InstallmentPercent\" , \"OthersOnLoan\" , \"EmploymentDuration\" , \"Sex\" , \"CurrentResidenceDuration\" , \"OwnsProperty\" , \"Age\" , \"Housing\" , \"Job\" , \"Dependents\" , \"Telephone\" , \"ForeignWorker\" , \"CheckingStatus\" , \"CreditHistory\" , \"ExistingSavings\" , \"InstallmentPlans\" , \"ExistingCreditsCount\" ], \"values\" : [ [ \"1\" , 13 , \"car_new\" , 1343 , 2 , \"none\" , \"1_to_4\" , \"female\" , 3 , \"savings_insurance\" , 46 , \"own\" , \"skilled\" , 1 , \"none\" , \"yes\" , \"no_checking\" , \"credits_paid_to_date\" , \"100_to_500\" , \"none\" , 2 ] ] } ] } Jupyter Spark Model { \"input_data\" : [{ \"fields\" : [ \"CheckingStatus\" , \"LoanDuration\" , \"CreditHistory\" , \"LoanPurpose\" , \"LoanAmount\" , \"ExistingSavings\" , \"EmploymentDuration\" , \"InstallmentPercent\" , \"Sex\" , \"OthersOnLoan\" , \"CurrentResidenceDuration\" , \"OwnsProperty\" , \"Age\" , \"InstallmentPlans\" , \"Housing\" , \"ExistingCreditsCount\" , \"Job\" , \"Dependents\" , \"Telephone\" , \"ForeignWorker\" ], \"values\" : [[ \"no_checking\" , 13 , \"credits_paid_to_date\" , \"car_new\" , 1343 , \"100_to_500\" , \"1_to_4\" , 2 , \"female\" , \"none\" , 3 , \"savings_insurance\" , 46 , \"none\" , \"own\" , 2 , \"skilled\" , 1 , \"none\" , \"yes\" ]] }]} Click the Predict button. The model will be called with the input data and the results will display in the Result window. Scroll down to the bottom of the result to see the prediction (i.e \"Risk\" or \"No Risk\"). Note: For some deployed models (for example AutoAI based models), you can provide the request payload using a generated form by clicking on the Provide input using form icon and providing values for the input fields of the form. If the form is not available for the model you deployed, the icon will not be present or will remain grayed out. Important: If you have completed this section and do not plan on completing the other optional deployment approaches below, please go ahead and cleanup your deployment. Follow the Cleanup Deployment instructions below. (Optional) Test Online Model Deployment using cURL \u00b6 Now that the model is deployed, we can also test it from external applications. One way to invoke the model API is using the cURL command. For simplicity, we will be using the IBM Cloud shell to run these cURL commands. If you have the IBM Cloud CLI and associated command line utilities installed locally, you are free to attempt these steps on your own machine. In a new browser window/tab, go to the IBM Cloud Home Page and click the terminal icon in the upper right-hand bar. The cloud shell will launch a new browser tab with a web terminal. In order to invoke the model endpoints, we need to authorize using an access token. To get an access token you will use the IBM Cloud API Key , which you created during the setup section. In the cloud shell window, run the following command to get a token to access the API. Replace <API Key> with the api key that you got from running above command. curl -X POST 'https://iam.cloud.ibm.com/identity/token' -H 'Content-Type: application/x-www-form-urlencoded' -H 'Accept: application/json' --data-urlencode 'grant_type=urn:ibm:params:oauth:grant-type:apikey' --data-urlencode 'apikey=<API Key>' A json string will be returned with a value for accessToken that will look similar to this: { \"access_token\" : \"AAAAAAAfakeACCESSTOKENNNNNNN\" , \"refresh_token\" : \"BBBBBBBBBBBFAKEREFRESHTOKENNNNNNNNNNNNN\" , \"token_type\" : \"Bearer\" , \"expires_in\" : 3600 , \"expiration\" : 1601317201 , \"scope\" : \"ibm openid\" } You will save the access token value shown after the access_token key in a temporary environment variable in your terminal. Copy the access token value (without the quotes) in the terminal and then use the following export command to save the \"accessToken\" to a variable called WML_AUTH_TOKEN . export WML_AUTH_TOKEN = <value-of-access-token> Back on the Cloud Pak for Data model deployment page, go to the API reference tab and copy the Endpoint value (an example endpoint would be: \"https://us-south.ml.cloud.ibm.com/ml/v4/deployments/<DEPLOYMENT_ID>/predictions?version=2020-09-01\" ). In your cloud shell terminal, save that endpoint to a variable named URL by exporting the value. ( Note: the URL should end with a version query parameter ). export WML_URL = <value-of-endpoint> Now run this curl command from the terminal to invoke the model with the same payload we used previousy: curl -k -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' --header \"Authorization: Bearer $WML_AUTH_TOKEN \" -d '{\"input_data\": [{\"fields\": [\"CustomerID\",\"LoanDuration\",\"LoanPurpose\",\"LoanAmount\",\"InstallmentPercent\",\"OthersOnLoan\",\"EmploymentDuration\",\"Sex\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"Housing\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\",\"CheckingStatus\",\"CreditHistory\",\"ExistingSavings\",\"InstallmentPlans\",\"ExistingCreditsCount\"],\"values\": [[\"1\",13,\"car_new\",1343,2,\"none\",\"1_to_4\",\"female\",3,\"savings_insurance\",46,\"own\",\"skilled\",1,\"none\",\"yes\",\"no_checking\",\"credits_paid_to_date\",\"100_to_500\",\"none\",2]]}]}' $WML_URL A json string will be returned with the response, including a prediction from the model (i.e a \"Risk\" or \"No Risk\" at the end indicating the prediction of this loan representing risk). Important: If you have completed this section and do not plan on completing the other optional deployment approaches below, please go ahead and cleanup your deployment. Follow the Cleanup Deployment instructions below. Cleanup Deployments \u00b6 You can clean up the deployments created for your models. To remove the deployment: Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, click the Assets tab and in the 'Model' table, click on the model name that you previousely promoted and created deployments against. Under 'Deployment Types', click on Online to view the online deployments you have created for this model. In the table on the main panel, click on the three vertical dots at the right of the row for the online deployment you created. Select the Delete option from the menu. Note: The vertical dots are hidden until you hover over them with your mouse In the subsequent pop up window, click on the Delete button to confirm you want to delete this deployment. You can follow the same process to delete other deployments as needed. Conclusion \u00b6 Congratulations. You've completed this lab and seen how to create and test online deployments for your machine learning models. Feel free to explore the additional (optional) deployment labs, which show you how to: Create and test a batch deployments. Integrate the online deployment to an application.","title":"Creating and Testing an Online Endpoint"},{"location":"ml-model-deployment/online-model-deployment/#machine-learning-model-online-deployment-and-scoring","text":"In this module, we will learn how to deploy our Machine Learning models. By doing so, we make them available for use in production such that applications and business processes can derive insights from them. There are several types of deployments available ( depending on the model framework used ). In this lab, we will explore: Online Deployments - Allows you to run the model on data in real-time, as data is received by a web service. This lab will build an online deployment and test the model endpoint using both the built in testing tool as well as external testing tools. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. If you would have any issues, check the FAQ section .","title":"Machine Learning Model Online Deployment and Scoring"},{"location":"ml-model-deployment/online-model-deployment/#create-online-model-deployment","text":"After a model has been created, saved and promoted to our deployment space, we can proceed to deploying the model. For this section, we will be creating an online deployment. This type of deployment will make an instance of the model available to make predictions in real time via an API. Although we will use the Cloud Pak for Data UI to deploy the model, the same can be done programmatically. Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, click the Assets tab and in the 'Models' table, find the model name for the model you previously built and now want to create a deployment against. Use your mouse to hover over the right side of that table row and click the Deploy rocket icon (the icons are not visible by default until you hover over them). Note: There may be more than one model listed in them 'Models' section. This can happen if you have run the Jupyter notebook that creates SparkML models more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. On the Create a deployment screen, choose Online for the Deployment Type , give the Deployment a name and optionally a description and click the Create button. Click on the Deployments tab. The new deployment status will show as In progress and then switch to Deployed when it is complete.","title":"Create Online Model Deployment"},{"location":"ml-model-deployment/online-model-deployment/#test-online-model-deployment","text":"Cloud Pak for Data offers tools to quickly test out Watson Machine Learning models. We begin with the built-in tooling. From the Model deployment page, once the deployment status shows as Deployed , click on the name of your deployment. The deployment API reference tab shows how to use the model using cURL , Java , Javascript , Python , and Scala . To get to the built-in test tool, click on the Test tab and then click on the Provide input data as JSON icon. Copy and paste the following data objects into the Body panel (replace the text that was in the input panel). Note: Click the tab appropriate for the model you are testing (either an AutoAI model or the SparkML one built using the Jupyter notebook). Also make sure the input below is the only content in the field. Do not append it to the default content { \"input_data\": [.......] } that may already be in the test input panel. AutoAI Model { \"input_data\" : [ { \"fields\" : [ \"CustomerID\" , \"LoanDuration\" , \"LoanPurpose\" , \"LoanAmount\" , \"InstallmentPercent\" , \"OthersOnLoan\" , \"EmploymentDuration\" , \"Sex\" , \"CurrentResidenceDuration\" , \"OwnsProperty\" , \"Age\" , \"Housing\" , \"Job\" , \"Dependents\" , \"Telephone\" , \"ForeignWorker\" , \"CheckingStatus\" , \"CreditHistory\" , \"ExistingSavings\" , \"InstallmentPlans\" , \"ExistingCreditsCount\" ], \"values\" : [ [ \"1\" , 13 , \"car_new\" , 1343 , 2 , \"none\" , \"1_to_4\" , \"female\" , 3 , \"savings_insurance\" , 46 , \"own\" , \"skilled\" , 1 , \"none\" , \"yes\" , \"no_checking\" , \"credits_paid_to_date\" , \"100_to_500\" , \"none\" , 2 ] ] } ] } Jupyter Spark Model { \"input_data\" : [{ \"fields\" : [ \"CheckingStatus\" , \"LoanDuration\" , \"CreditHistory\" , \"LoanPurpose\" , \"LoanAmount\" , \"ExistingSavings\" , \"EmploymentDuration\" , \"InstallmentPercent\" , \"Sex\" , \"OthersOnLoan\" , \"CurrentResidenceDuration\" , \"OwnsProperty\" , \"Age\" , \"InstallmentPlans\" , \"Housing\" , \"ExistingCreditsCount\" , \"Job\" , \"Dependents\" , \"Telephone\" , \"ForeignWorker\" ], \"values\" : [[ \"no_checking\" , 13 , \"credits_paid_to_date\" , \"car_new\" , 1343 , \"100_to_500\" , \"1_to_4\" , 2 , \"female\" , \"none\" , 3 , \"savings_insurance\" , 46 , \"none\" , \"own\" , 2 , \"skilled\" , 1 , \"none\" , \"yes\" ]] }]} Click the Predict button. The model will be called with the input data and the results will display in the Result window. Scroll down to the bottom of the result to see the prediction (i.e \"Risk\" or \"No Risk\"). Note: For some deployed models (for example AutoAI based models), you can provide the request payload using a generated form by clicking on the Provide input using form icon and providing values for the input fields of the form. If the form is not available for the model you deployed, the icon will not be present or will remain grayed out. Important: If you have completed this section and do not plan on completing the other optional deployment approaches below, please go ahead and cleanup your deployment. Follow the Cleanup Deployment instructions below.","title":"Test Online Model Deployment"},{"location":"ml-model-deployment/online-model-deployment/#optional-test-online-model-deployment-using-curl","text":"Now that the model is deployed, we can also test it from external applications. One way to invoke the model API is using the cURL command. For simplicity, we will be using the IBM Cloud shell to run these cURL commands. If you have the IBM Cloud CLI and associated command line utilities installed locally, you are free to attempt these steps on your own machine. In a new browser window/tab, go to the IBM Cloud Home Page and click the terminal icon in the upper right-hand bar. The cloud shell will launch a new browser tab with a web terminal. In order to invoke the model endpoints, we need to authorize using an access token. To get an access token you will use the IBM Cloud API Key , which you created during the setup section. In the cloud shell window, run the following command to get a token to access the API. Replace <API Key> with the api key that you got from running above command. curl -X POST 'https://iam.cloud.ibm.com/identity/token' -H 'Content-Type: application/x-www-form-urlencoded' -H 'Accept: application/json' --data-urlencode 'grant_type=urn:ibm:params:oauth:grant-type:apikey' --data-urlencode 'apikey=<API Key>' A json string will be returned with a value for accessToken that will look similar to this: { \"access_token\" : \"AAAAAAAfakeACCESSTOKENNNNNNN\" , \"refresh_token\" : \"BBBBBBBBBBBFAKEREFRESHTOKENNNNNNNNNNNNN\" , \"token_type\" : \"Bearer\" , \"expires_in\" : 3600 , \"expiration\" : 1601317201 , \"scope\" : \"ibm openid\" } You will save the access token value shown after the access_token key in a temporary environment variable in your terminal. Copy the access token value (without the quotes) in the terminal and then use the following export command to save the \"accessToken\" to a variable called WML_AUTH_TOKEN . export WML_AUTH_TOKEN = <value-of-access-token> Back on the Cloud Pak for Data model deployment page, go to the API reference tab and copy the Endpoint value (an example endpoint would be: \"https://us-south.ml.cloud.ibm.com/ml/v4/deployments/<DEPLOYMENT_ID>/predictions?version=2020-09-01\" ). In your cloud shell terminal, save that endpoint to a variable named URL by exporting the value. ( Note: the URL should end with a version query parameter ). export WML_URL = <value-of-endpoint> Now run this curl command from the terminal to invoke the model with the same payload we used previousy: curl -k -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' --header \"Authorization: Bearer $WML_AUTH_TOKEN \" -d '{\"input_data\": [{\"fields\": [\"CustomerID\",\"LoanDuration\",\"LoanPurpose\",\"LoanAmount\",\"InstallmentPercent\",\"OthersOnLoan\",\"EmploymentDuration\",\"Sex\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"Housing\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\",\"CheckingStatus\",\"CreditHistory\",\"ExistingSavings\",\"InstallmentPlans\",\"ExistingCreditsCount\"],\"values\": [[\"1\",13,\"car_new\",1343,2,\"none\",\"1_to_4\",\"female\",3,\"savings_insurance\",46,\"own\",\"skilled\",1,\"none\",\"yes\",\"no_checking\",\"credits_paid_to_date\",\"100_to_500\",\"none\",2]]}]}' $WML_URL A json string will be returned with the response, including a prediction from the model (i.e a \"Risk\" or \"No Risk\" at the end indicating the prediction of this loan representing risk). Important: If you have completed this section and do not plan on completing the other optional deployment approaches below, please go ahead and cleanup your deployment. Follow the Cleanup Deployment instructions below.","title":"(Optional) Test Online Model Deployment using cURL"},{"location":"ml-model-deployment/online-model-deployment/#cleanup-deployments","text":"You can clean up the deployments created for your models. To remove the deployment: Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, click the Assets tab and in the 'Model' table, click on the model name that you previousely promoted and created deployments against. Under 'Deployment Types', click on Online to view the online deployments you have created for this model. In the table on the main panel, click on the three vertical dots at the right of the row for the online deployment you created. Select the Delete option from the menu. Note: The vertical dots are hidden until you hover over them with your mouse In the subsequent pop up window, click on the Delete button to confirm you want to delete this deployment. You can follow the same process to delete other deployments as needed.","title":"Cleanup Deployments"},{"location":"ml-model-deployment/online-model-deployment/#conclusion","text":"Congratulations. You've completed this lab and seen how to create and test online deployments for your machine learning models. Feel free to explore the additional (optional) deployment labs, which show you how to: Create and test a batch deployments. Integrate the online deployment to an application.","title":"Conclusion"},{"location":"ml-model-deployment/sample-application-integration/","text":"Machine Learning Model Deployment Integration to Sample Applciation \u00b6 In this module, we will access an online model deployment through a REST API. This allows you to use your model for inference in any of your apps. For this workshop we'll be using a Python Flask application to collect information, score it against the model, and show the results. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. It's also assumed you have created an online deployment, if not, be sure to complete the online deployment . If you would have any issues, check the FAQ section . Running the Python Flask Application \u00b6 There are many ways of running python applications. We will cover two of them, first running as a python application on your machine, and next as a deployed application on IBM Cloud. Common Steps \u00b6 Regardless of which option we choose for deployment, we need to configure our Python application so it knows how to connect to our specific model. To do that follow these steps. Unzip the python app zip file that you downloaded in the setup section. Depending on your operating system the command to do this will differ, so an online search might be in order if you don't know how already! It's best practice to store secrets and configurations as environment variables, instead of hard-coding them in the code. Following this convention, we will store our API Key and model URL in a .env file. The key-value pairs in this files are treated as environment variables when the code runs. To create your environment file: Copy the env.sample file to .env . From a terminal (or command prompt), navigate to where you downloaded and unzipped the python app zip file. Run the following commands: cd python_app cp env.sample .env Edit .env to and fill in the MODEL_URL and API_TOKEN variables. API_TOKEN is your API Token that we created during the setup module. MODEL_URL is your online deployment's endpoint. If you need to get the endpoint again: Go to the (\u2630) hamburger menu > Deployments > View all spaces and then click the Spaces tab. Select the Deployment Space you created during the setup module Go to the Deployments tab and select the online deployment you created during the online deployment lab . Finally, you can find the Endpoint in the API reference section. Here is an example of a completed lines of the .env file. Your API_TOKEN and MODEL_URL will defer. # Copy this file to .env. # Edit the .env file with the required settings before starting the app. # 1. Required: Provide your web service URL for scoring. # E.g., MODEL_URL=https://<cluster_url>/v4/deployments/<deployment_space_guid>/predictions MODEL_URL = https://us-south.ml.cloud.ibm.com/ml/v4/deployments/012f3ebd-9885-4d1f-a720-xyzzzzff2a/predictions?version = 2020 -10-31 # 2. Required: fill in EITHER section A OR B below: # ### A: Authentication using API_TOKEN # Fill in your API Token. You don't need to update the TOKEN_REQUEST_URL # Example: # TOKEN_REQUEST_URL=https://iam.ng.bluemix.net/identity/token # API_TOKEN=<Your API Key> TOKEN_REQUEST_URL = https://iam.ng.bluemix.net/identity/token API_TOKEN = 0evvIIfebBsssbbbbbxxxxssddddsdpMiphntzhxqO And we're done! Now you can proceed to your favorite option below. Option 1: Running locally on your machine \u00b6 Choose this option if you want to run the Python Flask application locally on our machines. Note that this application will still access your deployed model in Cloud Pak for Data as a Service over the internet. Important pre-requisite: You need to have a working installation of Python 3.6 or above. Installing the dependencies \u00b6 You could run this Python application in your default python environment; however, the general recommendation for Python development is to use a virtual environments (see venv ). To install and initialize a virtual environment, use the venv module on Python 3. Initialize a virtual environment with venv . Run the following commands in a terminal (or command prompt): # Create the virtual environment using Python. # Note, it may be named python3 on your system. python -m venv venv # Python 3.X # Source the virtual environment. Use one of the two commands depending on your OS. source venv/bin/activate # Mac or Linux ./venv/Scripts/activate # Windows PowerShell TIP To terminate the virtual environment use the deactivate command. Next, to install the Python requirements, from a terminal (or command prompt) navigate to where you downloaded and unzipped the python app zip file. Run the following commands: cd python_app pip install -r requirements.txt Start and Test the Application \u00b6 Now we are ready to start our python application. From a terminal (or command prompt), run the following commands (inside the python application directory): # You might need to use python3 instead of python python creditriskapp.py Open your web browser and go to http://localhost:5000 . Either use the default values pre-filled in the input form, or modify the value and then click the Submit button. The python application will invoke the predictive model and a risk prediction & probability is returned: Feel free to test other input values, from your terminal enter ctrl + c to stop the Flask server when you are done. Option 2: Running on IBM Cloud \u00b6 Choose this option if you want to run the Python Flask application remotely in the IBM Cloud. You will deploy the application to your IBM Cloud account as a Cloud Foundry application. In this scenario, we will use the IBM Cloud Shell since it has the command line tools necessary to push applications. Note: If you do not want to use the cloud shell (web terminal). You will need a working installation of IBM Cloud CLI on your machine to push the application from your local machine to the cloud. ( See how to get started with the CLI ). Prepare Cloud Shell \u00b6 In a new browser window or tab, go to the IBM Cloud Home Page and click the terminal icon in the upper right-hand bar to launch a new cloud shell web terminal window. Wait for the web terminal to be ready and then run the following commands to download the python application to the web terminal. wget https://github.com/IBM/ddc-2021-development-to-production/raw/main/application/python_app.zip unzip python_app.zip cd python_app Next we need to copy the .env file content you've created in the Common Steps section above to the application in the web terminal. The easiest way to do this is to copy/paste them. Create the .env file by entering the following command: vi .env Note: The next set of steps use VI to edit the file. Although the exact VI commands are listed in the steps, feel free to explore these references: CSU Help docs or VI Intro Press the i key in the terminal to switch into insert mode (so you can edit the file). Copy the entire contents of the local .env file you created in the Common Steps section above and paste it into the terminal window. Write and exit the file in the terminal window by first pressing the escape key and then entering the following command: :wq Deploy Application \u00b6 [Optional] Configuring the application: You can inspect or change the deployment definitions for this application in the file named manifest.yml . As defined, your application will have a random URL. You can could change that by setting random-route: false and picking a unique name (for example append your initials to the existing name \"ddc-workshop-app-XYZ) in the name section. Ensure you are logged in on the IBM Cloud CLI. If you are running from the Cloud Shell, the CLI is automatically logged in for you, so you could skip to the next step. Otherwise, in the terminal, enter the command ibmcloud login to login and authenticate. (You can also use ibmcloud login --sso if your organization uses single-sign-on). Target the desired cloud foundry endpoint by using the following command: ibmcloud target --cf We are now ready to publish our application. In the terminal, navigate to python app directory and enter the command ibmcloud cf push to push your application to the cloud. Once it is complete you will see the URL for your application on the IBM Cloud. In a web browser, navigate to the URL that you received after publishing your application. Either use the default values pre-filled in the input form, or modify the value and then click the Submit button. The python application will invoke the predictive model and a risk prediction & probability is returned: Feel free to test other input values. And we are all done. We configured our application, logged in using the IBM Cloud cli, and published our application to the cloud. Conclusion \u00b6 Congratulations. You've completed this lab and seen how to integrate an online model deployment to a sample applications. Important: Please go ahead and cleanup your deployments. Follow the Cleanup Deployment instructions below.","title":"Integration with Sample Application"},{"location":"ml-model-deployment/sample-application-integration/#machine-learning-model-deployment-integration-to-sample-applciation","text":"In this module, we will access an online model deployment through a REST API. This allows you to use your model for inference in any of your apps. For this workshop we'll be using a Python Flask application to collect information, score it against the model, and show the results. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. It's also assumed you have created an online deployment, if not, be sure to complete the online deployment . If you would have any issues, check the FAQ section .","title":"Machine Learning Model Deployment Integration to Sample Applciation"},{"location":"ml-model-deployment/sample-application-integration/#running-the-python-flask-application","text":"There are many ways of running python applications. We will cover two of them, first running as a python application on your machine, and next as a deployed application on IBM Cloud.","title":"Running the Python Flask Application"},{"location":"ml-model-deployment/sample-application-integration/#common-steps","text":"Regardless of which option we choose for deployment, we need to configure our Python application so it knows how to connect to our specific model. To do that follow these steps. Unzip the python app zip file that you downloaded in the setup section. Depending on your operating system the command to do this will differ, so an online search might be in order if you don't know how already! It's best practice to store secrets and configurations as environment variables, instead of hard-coding them in the code. Following this convention, we will store our API Key and model URL in a .env file. The key-value pairs in this files are treated as environment variables when the code runs. To create your environment file: Copy the env.sample file to .env . From a terminal (or command prompt), navigate to where you downloaded and unzipped the python app zip file. Run the following commands: cd python_app cp env.sample .env Edit .env to and fill in the MODEL_URL and API_TOKEN variables. API_TOKEN is your API Token that we created during the setup module. MODEL_URL is your online deployment's endpoint. If you need to get the endpoint again: Go to the (\u2630) hamburger menu > Deployments > View all spaces and then click the Spaces tab. Select the Deployment Space you created during the setup module Go to the Deployments tab and select the online deployment you created during the online deployment lab . Finally, you can find the Endpoint in the API reference section. Here is an example of a completed lines of the .env file. Your API_TOKEN and MODEL_URL will defer. # Copy this file to .env. # Edit the .env file with the required settings before starting the app. # 1. Required: Provide your web service URL for scoring. # E.g., MODEL_URL=https://<cluster_url>/v4/deployments/<deployment_space_guid>/predictions MODEL_URL = https://us-south.ml.cloud.ibm.com/ml/v4/deployments/012f3ebd-9885-4d1f-a720-xyzzzzff2a/predictions?version = 2020 -10-31 # 2. Required: fill in EITHER section A OR B below: # ### A: Authentication using API_TOKEN # Fill in your API Token. You don't need to update the TOKEN_REQUEST_URL # Example: # TOKEN_REQUEST_URL=https://iam.ng.bluemix.net/identity/token # API_TOKEN=<Your API Key> TOKEN_REQUEST_URL = https://iam.ng.bluemix.net/identity/token API_TOKEN = 0evvIIfebBsssbbbbbxxxxssddddsdpMiphntzhxqO And we're done! Now you can proceed to your favorite option below.","title":"Common Steps"},{"location":"ml-model-deployment/sample-application-integration/#option-1-running-locally-on-your-machine","text":"Choose this option if you want to run the Python Flask application locally on our machines. Note that this application will still access your deployed model in Cloud Pak for Data as a Service over the internet. Important pre-requisite: You need to have a working installation of Python 3.6 or above.","title":"Option 1: Running locally on your machine"},{"location":"ml-model-deployment/sample-application-integration/#installing-the-dependencies","text":"You could run this Python application in your default python environment; however, the general recommendation for Python development is to use a virtual environments (see venv ). To install and initialize a virtual environment, use the venv module on Python 3. Initialize a virtual environment with venv . Run the following commands in a terminal (or command prompt): # Create the virtual environment using Python. # Note, it may be named python3 on your system. python -m venv venv # Python 3.X # Source the virtual environment. Use one of the two commands depending on your OS. source venv/bin/activate # Mac or Linux ./venv/Scripts/activate # Windows PowerShell TIP To terminate the virtual environment use the deactivate command. Next, to install the Python requirements, from a terminal (or command prompt) navigate to where you downloaded and unzipped the python app zip file. Run the following commands: cd python_app pip install -r requirements.txt","title":"Installing the dependencies"},{"location":"ml-model-deployment/sample-application-integration/#start-and-test-the-application","text":"Now we are ready to start our python application. From a terminal (or command prompt), run the following commands (inside the python application directory): # You might need to use python3 instead of python python creditriskapp.py Open your web browser and go to http://localhost:5000 . Either use the default values pre-filled in the input form, or modify the value and then click the Submit button. The python application will invoke the predictive model and a risk prediction & probability is returned: Feel free to test other input values, from your terminal enter ctrl + c to stop the Flask server when you are done.","title":"Start and Test the Application"},{"location":"ml-model-deployment/sample-application-integration/#option-2-running-on-ibm-cloud","text":"Choose this option if you want to run the Python Flask application remotely in the IBM Cloud. You will deploy the application to your IBM Cloud account as a Cloud Foundry application. In this scenario, we will use the IBM Cloud Shell since it has the command line tools necessary to push applications. Note: If you do not want to use the cloud shell (web terminal). You will need a working installation of IBM Cloud CLI on your machine to push the application from your local machine to the cloud. ( See how to get started with the CLI ).","title":"Option 2: Running on IBM Cloud"},{"location":"ml-model-deployment/sample-application-integration/#prepare-cloud-shell","text":"In a new browser window or tab, go to the IBM Cloud Home Page and click the terminal icon in the upper right-hand bar to launch a new cloud shell web terminal window. Wait for the web terminal to be ready and then run the following commands to download the python application to the web terminal. wget https://github.com/IBM/ddc-2021-development-to-production/raw/main/application/python_app.zip unzip python_app.zip cd python_app Next we need to copy the .env file content you've created in the Common Steps section above to the application in the web terminal. The easiest way to do this is to copy/paste them. Create the .env file by entering the following command: vi .env Note: The next set of steps use VI to edit the file. Although the exact VI commands are listed in the steps, feel free to explore these references: CSU Help docs or VI Intro Press the i key in the terminal to switch into insert mode (so you can edit the file). Copy the entire contents of the local .env file you created in the Common Steps section above and paste it into the terminal window. Write and exit the file in the terminal window by first pressing the escape key and then entering the following command: :wq","title":"Prepare Cloud Shell"},{"location":"ml-model-deployment/sample-application-integration/#deploy-application","text":"[Optional] Configuring the application: You can inspect or change the deployment definitions for this application in the file named manifest.yml . As defined, your application will have a random URL. You can could change that by setting random-route: false and picking a unique name (for example append your initials to the existing name \"ddc-workshop-app-XYZ) in the name section. Ensure you are logged in on the IBM Cloud CLI. If you are running from the Cloud Shell, the CLI is automatically logged in for you, so you could skip to the next step. Otherwise, in the terminal, enter the command ibmcloud login to login and authenticate. (You can also use ibmcloud login --sso if your organization uses single-sign-on). Target the desired cloud foundry endpoint by using the following command: ibmcloud target --cf We are now ready to publish our application. In the terminal, navigate to python app directory and enter the command ibmcloud cf push to push your application to the cloud. Once it is complete you will see the URL for your application on the IBM Cloud. In a web browser, navigate to the URL that you received after publishing your application. Either use the default values pre-filled in the input form, or modify the value and then click the Submit button. The python application will invoke the predictive model and a risk prediction & probability is returned: Feel free to test other input values. And we are all done. We configured our application, logged in using the IBM Cloud cli, and published our application to the cloud.","title":"Deploy Application"},{"location":"ml-model-deployment/sample-application-integration/#conclusion","text":"Congratulations. You've completed this lab and seen how to integrate an online model deployment to a sample applications. Important: Please go ahead and cleanup your deployments. Follow the Cleanup Deployment instructions below.","title":"Conclusion"},{"location":"ml-model-development-autoai/","text":"Machine Learning Model Development with AutoAI \u00b6 There are two labs in this module: AutoAI Experiment - Create a machine learning model using AutoAI. See how to create an experiment, explore the results and save a model & generated notebook. AutoAI Generated Notebooks - Once you have created an AutoAI experiment and saved a generated notebook. Run the notebook to interact with AutoAI programmatically.","title":"Machine Learning Model Development with AutoAI"},{"location":"ml-model-development-autoai/#machine-learning-model-development-with-autoai","text":"There are two labs in this module: AutoAI Experiment - Create a machine learning model using AutoAI. See how to create an experiment, explore the results and save a model & generated notebook. AutoAI Generated Notebooks - Once you have created an AutoAI experiment and saved a generated notebook. Run the notebook to interact with AutoAI programmatically.","title":"Machine Learning Model Development with AutoAI"},{"location":"ml-model-development-autoai/running-autoai-experiment/","text":"Automate model building with AutoAI \u00b6 In this module, we'll learn how to use AutoAI . The AutoAI tool is a capability that automates various tasks to ease the workflow for data scientists that are creating machine learning models. AutoAI automatically analyzes your data and generates candidate model pipelines customized for your predictive modeling problem. These model pipelines are created iteratively as AutoAI analyzes your dataset and discovers data transformations, algorithms, and parameter settings that work best for your problem setting. This section is broken up into the following steps: Run AutoAI Experiment Save and Promote AutoAI Model Save AutoAI Notebooks Conclusion Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. Create an AutoAI Experiment \u00b6 Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. To start the AutoAI experiment, click the Add to project + button from the top of the page and select the AutoAI experiment option. Give the AutoAI experiment any name you like. The associated Watson Machine Learning Service Instance should already be populated for you. If not, please select the one that you created in the setup section from the drop down. Then click the Create button. To configure the experiment, we must first give it the dataset that will be used to train the machine learning model. Click on the Select from project button to point to a dataset in your project. Now we can select the training CSV file. If you completed the previous Data Processing with Data Refinery lab module where you generated a single shaped CSV file from a refinery job, select that CSV file (the name will be whatever you selected in that module, for example: credit_risk_shaped ). If you did not complete that module, go ahead and select the german_credit_data.csv file that was preloaded in the project. Then click the Select asset button. If you are presented with a prompt to create a time series forecast, click the No button. Now, we will need to indicate what we want the model to predict. Under What do you want to predict? panel, select the Prediction column as Risk . AutoAI will set up defaults values for the experiment based on the dataset and the column selected for the prediction. This includes the type of model to build, the metrics to optimize against, the test/train split, etc. To view/change these values, click the Experiment settings button. Click on the Data source tab and scroll down to the Select features to include section. Deselect the checkbox for the CustomerID column name. This will remove the customer ID column from being used as a feature for the model. Although we could change other aspects of the experiment, we will accept the remaining default values and click the Save settings button. ( Feel free to explore the other possible settings before clicking the save button ). To start the experiment, click on the Run experiment button. The AutoAI experiment will now run. AutoAI will run through steps to prepare the dataset, split the dataset into training / evaluation groups and then find the best performing algorithms / estimators for the type of model. It will then build the following series of candidate pipelines for each of the top N performing algorithms (where N is a number chosen in the configuration which defaults to 2): Baseline model (Pipeline 1) Hyperparameter optimization (Pipeline 2) Automated feature engineering (Pipeline 3) Hyperparameter optimization on top of engineered features (Pipeline 4) The UI will show progress as different algorithms/evaluators are selected and as different pipelines are created and evaluated. You can view the performance of the pipelines that have completed by expanding each pipeline section in the leaderboard. The experiment can take several minutes to complete. Upon completion you will see a message that the pipelines have been created. Do not proceed to the next section until the experiment completes. Save and Promote AutoAI Model \u00b6 Once the experiment completes, you can explore the various pipelines and options in the UI. Some of the options available are to see a comparison of the pipelines, to change the ranking based on a different performance metric, to see a log of the experiment, or to see the ranked listing of the pipelines (ranking based on the optimization metric in your experiment, in this case, accuracy). Scroll down to see the Pipeline leaderboard . The top performing pipeline is in the first rank. The next step is to select the model that gives the best result and view its performance. In this case, Pipeline 3 gave the best result for our experiment. You can view the detailed results by clicking the corresponding pipeline name from the leaderboard: The model evaluation page will show metrics for the experiment, confusion matrix, feature transformations that were performed (if any), which features contribute to the model, and more details of the pipeline. Optionally, feel free to click through these views of the pipeline details. In order to deploy this model, click on the Save as button. On the next scren, select the Model option. Keep the default name, add an optional description and tags, and click the Create button to save the model. You will see a notification to indicate that your model is saved to your project. Click the View in project link in the notification to go to the saved model. ( Alternatively, if you navigate back to your project assets tab by closing the pipeline and AutoAI experiment, you will see the saved model in the Models section, which you can click on to explore ). To make the model available to be deployed, we need to make it available in the deployment space you created during the setup module. Click on the Promote to deployment space : Select the deployment space that was created as part of the setup module as the Target space and click Promote . You will be brought back to your project assets page and see a notification that the model was promoted to the deployment space succesfully. Feel free to close that notification. Save AutoAI notebooks \u00b6 In addition to saving models, we now have the ability to save a Jupyter notebook with working code. This can be the code representation of the AutoAI experiment (with access to all of the generated pipelines), or it can be the implementation of a specific pipeline. Let's explore how we can save both the experiment and a pipeline as a notebook. To save the AutoAI experiment as a notebook, navigate back to the AutoAI experiment overview page by clicking on the AutoAI experiment name in your project assets page (under the AutoAI experiments section). Click on the Save code link in the 'Progress map' section of the UI. Leave the default name and click the Create button to save the experiment as a notebook. To save the AutoAI pipeline as a notebook, from the experiment overview page, scroll down to the pipeline leader board section. Then hover over the right side of one of the other pipelines and click the resulting Save as button. For sake of demonstration, we are going to save a pipeline for an algorithm that is different from the model we saved in the previous section (i.e in this case Pipeline 7). Choose the Notebook tile, accept the default name, add an optional description, and click the Create button. You will receive a notification to indicate that your notebook is saved to your project. Close the notification and then click on your project name in the project path at the top of the screen. The notebook will be saved to your project, and can be examined in detail, changed and modified, and used to create a new model. See the instructions in the Modifying and running an AutoAI generated notebook lab for details. Conclusion \u00b6 Congratulation. We have now successfully created a highly optimized machine learning model using AutoAI and prepared it for deployment. In this section we covered one approach to building machine learning models on Cloud Pak for Data as a Service. We have seen how AutoAI helps to find an optimal model by automating tasks such as: Data Wrangling Algorithm Evaluation & Selection Feature Engineering Hyperparameter Optimization","title":"Run an AutoAI Experiment"},{"location":"ml-model-development-autoai/running-autoai-experiment/#automate-model-building-with-autoai","text":"In this module, we'll learn how to use AutoAI . The AutoAI tool is a capability that automates various tasks to ease the workflow for data scientists that are creating machine learning models. AutoAI automatically analyzes your data and generates candidate model pipelines customized for your predictive modeling problem. These model pipelines are created iteratively as AutoAI analyzes your dataset and discovers data transformations, algorithms, and parameter settings that work best for your problem setting. This section is broken up into the following steps: Run AutoAI Experiment Save and Promote AutoAI Model Save AutoAI Notebooks Conclusion Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space.","title":"Automate model building with AutoAI"},{"location":"ml-model-development-autoai/running-autoai-experiment/#create-an-autoai-experiment","text":"Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. To start the AutoAI experiment, click the Add to project + button from the top of the page and select the AutoAI experiment option. Give the AutoAI experiment any name you like. The associated Watson Machine Learning Service Instance should already be populated for you. If not, please select the one that you created in the setup section from the drop down. Then click the Create button. To configure the experiment, we must first give it the dataset that will be used to train the machine learning model. Click on the Select from project button to point to a dataset in your project. Now we can select the training CSV file. If you completed the previous Data Processing with Data Refinery lab module where you generated a single shaped CSV file from a refinery job, select that CSV file (the name will be whatever you selected in that module, for example: credit_risk_shaped ). If you did not complete that module, go ahead and select the german_credit_data.csv file that was preloaded in the project. Then click the Select asset button. If you are presented with a prompt to create a time series forecast, click the No button. Now, we will need to indicate what we want the model to predict. Under What do you want to predict? panel, select the Prediction column as Risk . AutoAI will set up defaults values for the experiment based on the dataset and the column selected for the prediction. This includes the type of model to build, the metrics to optimize against, the test/train split, etc. To view/change these values, click the Experiment settings button. Click on the Data source tab and scroll down to the Select features to include section. Deselect the checkbox for the CustomerID column name. This will remove the customer ID column from being used as a feature for the model. Although we could change other aspects of the experiment, we will accept the remaining default values and click the Save settings button. ( Feel free to explore the other possible settings before clicking the save button ). To start the experiment, click on the Run experiment button. The AutoAI experiment will now run. AutoAI will run through steps to prepare the dataset, split the dataset into training / evaluation groups and then find the best performing algorithms / estimators for the type of model. It will then build the following series of candidate pipelines for each of the top N performing algorithms (where N is a number chosen in the configuration which defaults to 2): Baseline model (Pipeline 1) Hyperparameter optimization (Pipeline 2) Automated feature engineering (Pipeline 3) Hyperparameter optimization on top of engineered features (Pipeline 4) The UI will show progress as different algorithms/evaluators are selected and as different pipelines are created and evaluated. You can view the performance of the pipelines that have completed by expanding each pipeline section in the leaderboard. The experiment can take several minutes to complete. Upon completion you will see a message that the pipelines have been created. Do not proceed to the next section until the experiment completes.","title":"Create an AutoAI Experiment"},{"location":"ml-model-development-autoai/running-autoai-experiment/#save-and-promote-autoai-model","text":"Once the experiment completes, you can explore the various pipelines and options in the UI. Some of the options available are to see a comparison of the pipelines, to change the ranking based on a different performance metric, to see a log of the experiment, or to see the ranked listing of the pipelines (ranking based on the optimization metric in your experiment, in this case, accuracy). Scroll down to see the Pipeline leaderboard . The top performing pipeline is in the first rank. The next step is to select the model that gives the best result and view its performance. In this case, Pipeline 3 gave the best result for our experiment. You can view the detailed results by clicking the corresponding pipeline name from the leaderboard: The model evaluation page will show metrics for the experiment, confusion matrix, feature transformations that were performed (if any), which features contribute to the model, and more details of the pipeline. Optionally, feel free to click through these views of the pipeline details. In order to deploy this model, click on the Save as button. On the next scren, select the Model option. Keep the default name, add an optional description and tags, and click the Create button to save the model. You will see a notification to indicate that your model is saved to your project. Click the View in project link in the notification to go to the saved model. ( Alternatively, if you navigate back to your project assets tab by closing the pipeline and AutoAI experiment, you will see the saved model in the Models section, which you can click on to explore ). To make the model available to be deployed, we need to make it available in the deployment space you created during the setup module. Click on the Promote to deployment space : Select the deployment space that was created as part of the setup module as the Target space and click Promote . You will be brought back to your project assets page and see a notification that the model was promoted to the deployment space succesfully. Feel free to close that notification.","title":"Save and Promote AutoAI Model"},{"location":"ml-model-development-autoai/running-autoai-experiment/#save-autoai-notebooks","text":"In addition to saving models, we now have the ability to save a Jupyter notebook with working code. This can be the code representation of the AutoAI experiment (with access to all of the generated pipelines), or it can be the implementation of a specific pipeline. Let's explore how we can save both the experiment and a pipeline as a notebook. To save the AutoAI experiment as a notebook, navigate back to the AutoAI experiment overview page by clicking on the AutoAI experiment name in your project assets page (under the AutoAI experiments section). Click on the Save code link in the 'Progress map' section of the UI. Leave the default name and click the Create button to save the experiment as a notebook. To save the AutoAI pipeline as a notebook, from the experiment overview page, scroll down to the pipeline leader board section. Then hover over the right side of one of the other pipelines and click the resulting Save as button. For sake of demonstration, we are going to save a pipeline for an algorithm that is different from the model we saved in the previous section (i.e in this case Pipeline 7). Choose the Notebook tile, accept the default name, add an optional description, and click the Create button. You will receive a notification to indicate that your notebook is saved to your project. Close the notification and then click on your project name in the project path at the top of the screen. The notebook will be saved to your project, and can be examined in detail, changed and modified, and used to create a new model. See the instructions in the Modifying and running an AutoAI generated notebook lab for details.","title":"Save AutoAI notebooks"},{"location":"ml-model-development-autoai/running-autoai-experiment/#conclusion","text":"Congratulation. We have now successfully created a highly optimized machine learning model using AutoAI and prepared it for deployment. In this section we covered one approach to building machine learning models on Cloud Pak for Data as a Service. We have seen how AutoAI helps to find an optimal model by automating tasks such as: Data Wrangling Algorithm Evaluation & Selection Feature Engineering Hyperparameter Optimization","title":"Conclusion"},{"location":"ml-model-development-autoai/running-autoai-notebook/","text":"Modifying and Running an AutoAI Generated Notebook \u00b6 In this module, we'll explore how to run the AutoAI generated jupyter notebooks. The notebooks allows you to interact with the experiment or pipeline (pipeline notebooks are currently only supported for single data source, non-time series experiments) programmatically. This exploration will help you understand the transformations applied to build a model, and can serve as a starting point for further pipeline/model development. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. It is also assumed you have completed the AutoAI lab where you completed an AutoAI experiment and saved a pipeline as a Jupyter notebook which you will be exploring in this module. Open AutoAI Experiment Notebook \u00b6 Let's start by exploring the AutoAI experiment notebook. This notebook contains all the code to view the transformations and optimizations applied to create the model pipelines. It includes generated code to access experiment details/configuration, visualize each of the pipelines, compare pipelines, and deploy individual pipelines. Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the Auto AI experiment notebook (the exact name will be based on the name of the AutoAI experiment you ran but will end in - experiment notebook ). When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. Run Jupyter notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ([*]) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17]). The notebook generated is pre filled with Python code and is divided into the following sections: 1.0 Setup 2.0 Experiment Configuration 3.0 Watson Machine Learning Connection 4.0 Working with Completed AutoAI Experiment 5.0 Deploy and score 1.0 Setup \u00b6 This section installs the necessary packages and libraries to run the AutoAI experiment and scikit learn pipelines. Run the cell in this section as is. After the libraries are installed, you should restart the Jupyter kernel. Click on Kernel menu at the top of the page and select the Restart option from the menu. Note: If you do not restart the kernel, as you execute furtuer cells in the notebook you may see errors indicating there are missing libraries or missing specific version of libraries. 2.0 Experiment Configuration \u00b6 The first part of this section contains credentials to Cloud Object Storage through which the training data that was used to create the pipelines and the pipeline results are retrieved. Run this cell as is. The next cell contains the metadata of the experiment configuration. Run the cell as is. 3.0 Watson Machine Learning Connection \u00b6 This section will setup the credentials to the Machine Learning instance you provisoned. In the code cell, replace the variable value 'PUT_YOUR_KEY_HERE' with your the API key you created during the setup section. ( Note: Remember to keep the single quotes ). Once you've updated the api_key variable, run the two cells in this section. 4.0 Working with Completed AutoAI Experiment \u00b6 Within this section of the notebook, the first set of code cells extract the current experiment (i.e. pipeline_optimizer ) and then gets the pipelines from the experiment to display their performance in a table. Run the cells until you see the table output from the Pipeline Comparison cell. The next cell will retrieve a pipeline from the experiment. The generated code retrieves the top pipeline. Before running the cell, use a # to comment out the generated code: pipeline_model = pipeline_optimizer.get_pipeline() . Then copy the following code and paste it below the line you just commented out ( Note: your code cell should look like the code in the next screenshot ). pipeline_to_explore = 'INSERT_PIPELINE_NAME_HERE' pipeline_model = pipeline_optimizer . get_pipeline ( pipeline_name = pipeline_to_explore ) In the code you just inserted, replace the value INSERT_PIPELINE_NAME_HERE with one of the pipeline names you see in the first column of the summary table ( Note: Pick a pipeline name you haven't deployed in a previous lab. In the screenshot below, we are getting the pipeline Pipeline_7 ). Once you have updated this cell, go ahead and run it. Continue running the other cells in this section without modification, including the cell in the Visualize pipeline model section and the Preview pipeline model as python code section. The former cell output will display a graph of the pipeline. You can hover over any of the graph nodes to get information about the task being done in that step. The latter cell allows you to get the pipeline implementation as either lale wrapper code or directly sklearn pipeline representation. 5.0 Deploy and Score \u00b6 This section of the notebook contains code that deploys the pipeline model as a web service using Watson Machine Learning. This section requires users to enter a target_space_id of the deployment space where the model will be deployed. To get the deployment space ID for the space you created during the setup module, perform the following steps: In a new browser window or tab. Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. Click on the Manage tab and then click the copy icon next to the Space GUID . In the Deployment creation code cell, paste the space ID you just copied above as the value for the target_space_id variable. Also change the line: model=best_pipeline_name, to model=pipeline_to_explore,#best_pipeline_name, . This will deploy the pipeline we were exploring above to our deployment space. Go ahead and run the cell. Once the cell is executed, the call in the SDK will both save the model to the deployment space and create an online deployment. Run the remaining cells in the notebook. These cells provide some samples of using the SDK to modify and run AutoAI experiments programmatically. (Optional) Open AutoAI Pipeline Notebook \u00b6 For further exploration, you can optionally run the AutoAI pipeline notebook. This notebook contains the code to implement a specific pipeline from the experiment. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the Auto AI pipeline notebook (the exact name will be based on the name of the AutoAI experiment you ran followed by the pipeline number). When the Jupyter notebook is loaded and the kernel is ready, you can go ahead and run all the cells in the notebook. This notebook will build the pipeline and train it using the the data from the experiment. Note that the exact implementation will vary based on which pipeline you saved as a notebook. Some pipelines will have different features (feature engineering, HPO) and train different evaluators. Stop the Environment \u00b6 In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Conclusion \u00b6 In this part of the lab, we examined and ran a Jupyter notebook that was generated as the result of an AutoAI experiment. Feel free to modify and re-run the notebook, making any changes that you are comfortable with.","title":"Run an AutoAI Notebook"},{"location":"ml-model-development-autoai/running-autoai-notebook/#modifying-and-running-an-autoai-generated-notebook","text":"In this module, we'll explore how to run the AutoAI generated jupyter notebooks. The notebooks allows you to interact with the experiment or pipeline (pipeline notebooks are currently only supported for single data source, non-time series experiments) programmatically. This exploration will help you understand the transformations applied to build a model, and can serve as a starting point for further pipeline/model development. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. It is also assumed you have completed the AutoAI lab where you completed an AutoAI experiment and saved a pipeline as a Jupyter notebook which you will be exploring in this module.","title":"Modifying and Running an AutoAI Generated Notebook"},{"location":"ml-model-development-autoai/running-autoai-notebook/#open-autoai-experiment-notebook","text":"Let's start by exploring the AutoAI experiment notebook. This notebook contains all the code to view the transformations and optimizations applied to create the model pipelines. It includes generated code to access experiment details/configuration, visualize each of the pipelines, compare pipelines, and deploy individual pipelines. Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the Auto AI experiment notebook (the exact name will be based on the name of the AutoAI experiment you ran but will end in - experiment notebook ). When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"Open AutoAI Experiment Notebook"},{"location":"ml-model-development-autoai/running-autoai-notebook/#run-jupyter-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ([*]) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17]). The notebook generated is pre filled with Python code and is divided into the following sections: 1.0 Setup 2.0 Experiment Configuration 3.0 Watson Machine Learning Connection 4.0 Working with Completed AutoAI Experiment 5.0 Deploy and score","title":"Run Jupyter notebook"},{"location":"ml-model-development-autoai/running-autoai-notebook/#10-setup","text":"This section installs the necessary packages and libraries to run the AutoAI experiment and scikit learn pipelines. Run the cell in this section as is. After the libraries are installed, you should restart the Jupyter kernel. Click on Kernel menu at the top of the page and select the Restart option from the menu. Note: If you do not restart the kernel, as you execute furtuer cells in the notebook you may see errors indicating there are missing libraries or missing specific version of libraries.","title":"1.0 Setup"},{"location":"ml-model-development-autoai/running-autoai-notebook/#20-experiment-configuration","text":"The first part of this section contains credentials to Cloud Object Storage through which the training data that was used to create the pipelines and the pipeline results are retrieved. Run this cell as is. The next cell contains the metadata of the experiment configuration. Run the cell as is.","title":"2.0 Experiment Configuration"},{"location":"ml-model-development-autoai/running-autoai-notebook/#30-watson-machine-learning-connection","text":"This section will setup the credentials to the Machine Learning instance you provisoned. In the code cell, replace the variable value 'PUT_YOUR_KEY_HERE' with your the API key you created during the setup section. ( Note: Remember to keep the single quotes ). Once you've updated the api_key variable, run the two cells in this section.","title":"3.0 Watson Machine Learning Connection"},{"location":"ml-model-development-autoai/running-autoai-notebook/#40-working-with-completed-autoai-experiment","text":"Within this section of the notebook, the first set of code cells extract the current experiment (i.e. pipeline_optimizer ) and then gets the pipelines from the experiment to display their performance in a table. Run the cells until you see the table output from the Pipeline Comparison cell. The next cell will retrieve a pipeline from the experiment. The generated code retrieves the top pipeline. Before running the cell, use a # to comment out the generated code: pipeline_model = pipeline_optimizer.get_pipeline() . Then copy the following code and paste it below the line you just commented out ( Note: your code cell should look like the code in the next screenshot ). pipeline_to_explore = 'INSERT_PIPELINE_NAME_HERE' pipeline_model = pipeline_optimizer . get_pipeline ( pipeline_name = pipeline_to_explore ) In the code you just inserted, replace the value INSERT_PIPELINE_NAME_HERE with one of the pipeline names you see in the first column of the summary table ( Note: Pick a pipeline name you haven't deployed in a previous lab. In the screenshot below, we are getting the pipeline Pipeline_7 ). Once you have updated this cell, go ahead and run it. Continue running the other cells in this section without modification, including the cell in the Visualize pipeline model section and the Preview pipeline model as python code section. The former cell output will display a graph of the pipeline. You can hover over any of the graph nodes to get information about the task being done in that step. The latter cell allows you to get the pipeline implementation as either lale wrapper code or directly sklearn pipeline representation.","title":"4.0 Working with Completed AutoAI Experiment"},{"location":"ml-model-development-autoai/running-autoai-notebook/#50-deploy-and-score","text":"This section of the notebook contains code that deploys the pipeline model as a web service using Watson Machine Learning. This section requires users to enter a target_space_id of the deployment space where the model will be deployed. To get the deployment space ID for the space you created during the setup module, perform the following steps: In a new browser window or tab. Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. Click on the Manage tab and then click the copy icon next to the Space GUID . In the Deployment creation code cell, paste the space ID you just copied above as the value for the target_space_id variable. Also change the line: model=best_pipeline_name, to model=pipeline_to_explore,#best_pipeline_name, . This will deploy the pipeline we were exploring above to our deployment space. Go ahead and run the cell. Once the cell is executed, the call in the SDK will both save the model to the deployment space and create an online deployment. Run the remaining cells in the notebook. These cells provide some samples of using the SDK to modify and run AutoAI experiments programmatically.","title":"5.0 Deploy and Score"},{"location":"ml-model-development-autoai/running-autoai-notebook/#optional-open-autoai-pipeline-notebook","text":"For further exploration, you can optionally run the AutoAI pipeline notebook. This notebook contains the code to implement a specific pipeline from the experiment. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the Auto AI pipeline notebook (the exact name will be based on the name of the AutoAI experiment you ran followed by the pipeline number). When the Jupyter notebook is loaded and the kernel is ready, you can go ahead and run all the cells in the notebook. This notebook will build the pipeline and train it using the the data from the experiment. Note that the exact implementation will vary based on which pipeline you saved as a notebook. Some pipelines will have different features (feature engineering, HPO) and train different evaluators.","title":"(Optional) Open AutoAI Pipeline Notebook"},{"location":"ml-model-development-autoai/running-autoai-notebook/#stop-the-environment","text":"In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window.","title":"Stop the Environment"},{"location":"ml-model-development-autoai/running-autoai-notebook/#conclusion","text":"In this part of the lab, we examined and ran a Jupyter notebook that was generated as the result of an AutoAI experiment. Feel free to modify and re-run the notebook, making any changes that you are comfortable with.","title":"Conclusion"},{"location":"ml-model-ops-and-versioning/","text":"Model Operations and Versioning \u00b6 There are two labs in this module: Updating Model Deployments - Update an online model deployment to expose a new model, without affecting the end users/consumers of the endpoint. Moving Assets Across Environments - Organizations will likely have multiple environments for development, testing and production to deploy their models. This lab shows how assets can be moved (i.e \"promoted\") from one space to another.","title":"Model Operations and Versioning"},{"location":"ml-model-ops-and-versioning/#model-operations-and-versioning","text":"There are two labs in this module: Updating Model Deployments - Update an online model deployment to expose a new model, without affecting the end users/consumers of the endpoint. Moving Assets Across Environments - Organizations will likely have multiple environments for development, testing and production to deploy their models. This lab shows how assets can be moved (i.e \"promoted\") from one space to another.","title":"Model Operations and Versioning"},{"location":"ml-model-ops-and-versioning/deployment-asset-operations/","text":"Moving Assets Between Spaces \u00b6 In this module, we will explore how we can move assets (i.e machine learning models) from one space to another space. Model operations and lifecycle management includes many different aspects that would need to be addressed, including, but not limited to: determining when a model needs to be replaced, testing models, comparing model performance, deployment testing (for example canary testing), and much more. In this lab we will focus just on the scenario of \"promoting\" a model asset from a deployment space that represents development to another space that could represent a staging/testing space. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the Model Deployment labs . You should have already created a model and promoted it to a deployment space which is what we will use in this lab. If you run into an any issues, check the FAQ section for common resolutions*. Create A New Deployment Space \u00b6 Cloud Pak for Data uses the concept of Deployment Spaces to configure and manage the deployment of a set of related deployable assets. These assets can be data files, machine learning models, etc. We have been working with a space created during the setup module through out the previous labs. Go the (\u2630) navigation menu, expand Deployment spaces and then select View all spaces . Click on the New deployment space button. Give your deployment space a unique name (i.e. TestProductionSpace ) and optional description. In the service drop downs, select the Cloud Object Storage instance that you had created during the setup module and select the Machine Learning Service instance associated with your IBM Cloud Pak for Data as a Service instance. Then click the Create button. Once the deployment space is created, you can click on View new space . Next, we need to get the deployment space ID for this space. Click on the Manage tab and then click the copy icon next to the Space GUID . Save this ID in a notepad, we will use it later in the lab. Run the Notebook \u00b6 The Jupyter notebook is already included as an asset in the project you imported during the setup module. Go the (\u2630) navigation menu and click on the Projects link and then click on the project you created during the setup. From the project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the model-asset-operations notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. Notebook sections \u00b6 With the notebook open, spend a minute looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Please note that some of the comments in the notebook are directions for you to modify specific sections of the code. These are written in red . Perform any changes necessary, as indicated in the cells, before executing them. Execute the cells one at a time. Paying attention to the comments / instructions for you to update variables. Section 1.0 Install required packages will install some of the libraries we are going to use in the notebook. Note that we upgrade the installed version of Watson Machine Learning Python Client. Ensure the output of the first code cell is that the python packages were successfully installed. Section 2.0 Instantiate Watson Machine Learning Client will setup the client to the Machine Learning instance / deployment spaces where the assets are deployed. In the first code cell, you will need to provide your IBM Cloud API key and Machine learning service location, which you should have from the setup module. Section 3.0 Get Assets from Development Deployment Space will export the assets (in this case machine learning models) we want to move to a new target space. It uses the export APIs to pull specific models to a file. In the second code cell of section 3.0 you will need to update the ID for your source deployment space (the space where the models are currently deployed). You could copy the ID from the output of the previous cell. In the second code cell of section 3.1 you will need to provide the ID of the model we are going to export out of this space. You could copy one of the model IDs from the output of the previous cell. Continue running the cells in this section to finish exporting the assets. Section 4.0 Push Assets to target Staging Deployment Space will import the assets (in this case machine learning models) we just exported to a new space. It uses the import APIs to push specific models to the space. In the second code cell of section 4.0 you will need to update the ID for the target deployment space (the space where the models are going to be deployed). You should have the ID from the section above, but if you dont just copy the ID from the output of the previous cell. Continue running the cells in this section to finish importing the assets. Section 5.0 Deploy and score the imported model is an OPTIONAL section that creates an online deployment for the newly imported model so that you could test the model. Model deployment has been explored as part of a separate module and is out of the scope for this lab. So the code to create the deployment and test has been set to a non-code type cell. If you want to attempt this section, you will need to change the cells to be Code type and execute them. As another optional exercise, you could navigate to new deployment space and see the model that has been imported into the space. Stop the Environment \u00b6 In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Conclusion \u00b6 Congratulations. You've completed this lab and have seen how to export assets from one deployment space and import them to another space. Although this is a contrived example, this approach can be used to setup and use different spaces to separate development from QA, staging, production, etc. It can even be incorporated into your CI/CD pipelines to aid in the iterative model operations and lifecycle.","title":"Promoting Assets Across Spaces"},{"location":"ml-model-ops-and-versioning/deployment-asset-operations/#moving-assets-between-spaces","text":"In this module, we will explore how we can move assets (i.e machine learning models) from one space to another space. Model operations and lifecycle management includes many different aspects that would need to be addressed, including, but not limited to: determining when a model needs to be replaced, testing models, comparing model performance, deployment testing (for example canary testing), and much more. In this lab we will focus just on the scenario of \"promoting\" a model asset from a deployment space that represents development to another space that could represent a staging/testing space. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the Model Deployment labs . You should have already created a model and promoted it to a deployment space which is what we will use in this lab. If you run into an any issues, check the FAQ section for common resolutions*.","title":"Moving Assets Between Spaces"},{"location":"ml-model-ops-and-versioning/deployment-asset-operations/#create-a-new-deployment-space","text":"Cloud Pak for Data uses the concept of Deployment Spaces to configure and manage the deployment of a set of related deployable assets. These assets can be data files, machine learning models, etc. We have been working with a space created during the setup module through out the previous labs. Go the (\u2630) navigation menu, expand Deployment spaces and then select View all spaces . Click on the New deployment space button. Give your deployment space a unique name (i.e. TestProductionSpace ) and optional description. In the service drop downs, select the Cloud Object Storage instance that you had created during the setup module and select the Machine Learning Service instance associated with your IBM Cloud Pak for Data as a Service instance. Then click the Create button. Once the deployment space is created, you can click on View new space . Next, we need to get the deployment space ID for this space. Click on the Manage tab and then click the copy icon next to the Space GUID . Save this ID in a notepad, we will use it later in the lab.","title":"Create A New Deployment Space"},{"location":"ml-model-ops-and-versioning/deployment-asset-operations/#run-the-notebook","text":"The Jupyter notebook is already included as an asset in the project you imported during the setup module. Go the (\u2630) navigation menu and click on the Projects link and then click on the project you created during the setup. From the project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the model-asset-operations notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"Run the Notebook"},{"location":"ml-model-ops-and-versioning/deployment-asset-operations/#notebook-sections","text":"With the notebook open, spend a minute looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ( [*] ) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17] ). Note: Please note that some of the comments in the notebook are directions for you to modify specific sections of the code. These are written in red . Perform any changes necessary, as indicated in the cells, before executing them. Execute the cells one at a time. Paying attention to the comments / instructions for you to update variables. Section 1.0 Install required packages will install some of the libraries we are going to use in the notebook. Note that we upgrade the installed version of Watson Machine Learning Python Client. Ensure the output of the first code cell is that the python packages were successfully installed. Section 2.0 Instantiate Watson Machine Learning Client will setup the client to the Machine Learning instance / deployment spaces where the assets are deployed. In the first code cell, you will need to provide your IBM Cloud API key and Machine learning service location, which you should have from the setup module. Section 3.0 Get Assets from Development Deployment Space will export the assets (in this case machine learning models) we want to move to a new target space. It uses the export APIs to pull specific models to a file. In the second code cell of section 3.0 you will need to update the ID for your source deployment space (the space where the models are currently deployed). You could copy the ID from the output of the previous cell. In the second code cell of section 3.1 you will need to provide the ID of the model we are going to export out of this space. You could copy one of the model IDs from the output of the previous cell. Continue running the cells in this section to finish exporting the assets. Section 4.0 Push Assets to target Staging Deployment Space will import the assets (in this case machine learning models) we just exported to a new space. It uses the import APIs to push specific models to the space. In the second code cell of section 4.0 you will need to update the ID for the target deployment space (the space where the models are going to be deployed). You should have the ID from the section above, but if you dont just copy the ID from the output of the previous cell. Continue running the cells in this section to finish importing the assets. Section 5.0 Deploy and score the imported model is an OPTIONAL section that creates an online deployment for the newly imported model so that you could test the model. Model deployment has been explored as part of a separate module and is out of the scope for this lab. So the code to create the deployment and test has been set to a non-code type cell. If you want to attempt this section, you will need to change the cells to be Code type and execute them. As another optional exercise, you could navigate to new deployment space and see the model that has been imported into the space.","title":"Notebook sections"},{"location":"ml-model-ops-and-versioning/deployment-asset-operations/#stop-the-environment","text":"In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window.","title":"Stop the Environment"},{"location":"ml-model-ops-and-versioning/deployment-asset-operations/#conclusion","text":"Congratulations. You've completed this lab and have seen how to export assets from one deployment space and import them to another space. Although this is a contrived example, this approach can be used to setup and use different spaces to separate development from QA, staging, production, etc. It can even be incorporated into your CI/CD pipelines to aid in the iterative model operations and lifecycle.","title":"Conclusion"},{"location":"ml-model-ops-and-versioning/deployment-model-updates/","text":"Updating your models and deployments \u00b6 In this module, we will explore how we can update a model or deployment once it has been released/created. Model operations and lifecycle management includes many different aspects that would need to be addressed, including, but not limited to: determining when a model needs to be replaced, testing models, comparing model performance, deployment testing (for example canary testing), and much more. In this lab we will focus just on the capabilities to update the model or the deployment, not on the methodology or testing approach. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the Machine Learning with AutoAI labs and the Model Deployment labs . You should have already created a model, promoted it to a deployment space and created an online deployment which is what we will update in this lab. If you run into an any issues, check the FAQ section for common resolutions. Updating a Deployment \u00b6 Once a deployment is created for a model, there may be consumers/clients of that model that depend on the endpoint to that deployment not changing. In this section we are going to update an existing deployment to use a new model asset (for example, when you have created a model with improved performance). Its important to note that in order to update a deployment, the framework of the new model must be compatible with the existing deployed model. Also, the input schema for the new model must match the deployed model. Note: If you have completed both of the AutoAI Labs (1) model development with the UI and (2)running the generated Jupyter notebook), then you will have two models promoted in your deployment space and can proceed to the next section. If you have not completed the second lab, you can follow the instructions at the bottom of this page to save/promote a second pipeline. Modify the Online Deployment \u00b6 Lets go to our deployment space to view our model assets and deployments. Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Choose the deployment space you setup previously by clicking on the name of your space. In the Assets tab you will see the models you promoted to this space. Lets check the deployment you created previously. Click on the Deployment tab and select the online deployment name you previously created. Notice in the deployment details the endpoint has an unique ID and the deployment is pointing to a specific model asset (in this case the Pipeline 3 model). Optionally, feel free to test the model by submitting a sample payload in the test panel as you did in the model deployment lab . Go back to the deployment space overview by clicking on the deployment space name in the navigation breadcrumbs on the top left. Now, on the Deployments tab. Click on the three vertical dots to the right of your online deployment. Then select the Replace asset option from the menu. You can now select the asset that will replace the existing model for this deployment. Again, one thing to note is that you should only select an asset that is using a framework compatible with the existing deployed model and that the input schema exists and matches between the new and deployed model. Otherwise, the deployment may fail. Click the checkbox next to the second model you promoted to this space (in this case, Pipeline 8). Then click the Replace button. The deployment will be updated to use the replaced model. This may take a few minutes, during which time you will see the status set to Updating . Once the update completes, you will receive a notification that the deployment has been updated and the status will change to Deployed . To validate the update, click on the deployment name. Notice that the endpoint has not changed (the unique id is still the same) but the associated asset has been updated to our new model. Optionally, feel free to test the model by submitting a sample payload in the test panel as you did in the model deployment lab . You may notice that the model makes a different prediction or have different scores from the previous model. Extra Instructions to Save and Promote a Second Model \u00b6 These additional instructions are provided to help you deploy a second model from an existing AutoAI experiment. You only need to run them if you do not currently have two models in your deployment space and you need a reminder on how to save and promote another pipeline from AutoAI, which you need to complete the lab above. To get started, we will need to identify the new model we want to use. Go the (\u2630) navigation menu, expand Projects and then click on your analytics project. Click on the completed AutoAI experiment you previously ran. Scroll down to see the Pipeline leaderboard . In this case, Pipeline 3 was the pipeline with the best result (on the cross validation set) for our experiment and the one we deployed during the previous lab. We will deploy a second pipeline using a different model algorithm. Click on the Save as button next for one of other pipelines. In this case, we are saving Pipeline 8. Note: The exact pipelines in your experiment may be different. The goal of this portion of the lab is to showcase how a deployment can be updated, so we can save any of the pipelines we have not previously promoted. Choose the Model tile, accept the default name or change it if you like. Add optional description or tags, and click Create . You will see a notification to indicate that your model is saved to your project. Click the View in project link in the notification to go to the saved model. ( Alternatively, if you navigate back to your project assets tab by closing the pipeline and AutoAI experiment, you will see the saved model in the Models section, which you can click on to explore ). To make the model available to be deployed, we need to make it available in the deployment space you created during the setup module. Click on the Promote to deployment space button. Select the deployment space that was created as part of the setup module as the Target space and click Promote . You will be brought back to your project assets page and see a notification that the model was promoted to the deployment space succesfully. Feel free to close that notification. Conclusion \u00b6 In this section we covered one approach to updating the deployment of a machine learning model. Although there are other aspects to managing and operating machine learning models in production, we have seen how to make changes to a model without impacting the model serving endpoint.","title":"Updating Model and Deployment Versions"},{"location":"ml-model-ops-and-versioning/deployment-model-updates/#updating-your-models-and-deployments","text":"In this module, we will explore how we can update a model or deployment once it has been released/created. Model operations and lifecycle management includes many different aspects that would need to be addressed, including, but not limited to: determining when a model needs to be replaced, testing models, comparing model performance, deployment testing (for example canary testing), and much more. In this lab we will focus just on the capabilities to update the model or the deployment, not on the methodology or testing approach. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the Machine Learning with AutoAI labs and the Model Deployment labs . You should have already created a model, promoted it to a deployment space and created an online deployment which is what we will update in this lab. If you run into an any issues, check the FAQ section for common resolutions.","title":"Updating your models and deployments"},{"location":"ml-model-ops-and-versioning/deployment-model-updates/#updating-a-deployment","text":"Once a deployment is created for a model, there may be consumers/clients of that model that depend on the endpoint to that deployment not changing. In this section we are going to update an existing deployment to use a new model asset (for example, when you have created a model with improved performance). Its important to note that in order to update a deployment, the framework of the new model must be compatible with the existing deployed model. Also, the input schema for the new model must match the deployed model. Note: If you have completed both of the AutoAI Labs (1) model development with the UI and (2)running the generated Jupyter notebook), then you will have two models promoted in your deployment space and can proceed to the next section. If you have not completed the second lab, you can follow the instructions at the bottom of this page to save/promote a second pipeline.","title":"Updating a Deployment"},{"location":"ml-model-ops-and-versioning/deployment-model-updates/#modify-the-online-deployment","text":"Lets go to our deployment space to view our model assets and deployments. Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Choose the deployment space you setup previously by clicking on the name of your space. In the Assets tab you will see the models you promoted to this space. Lets check the deployment you created previously. Click on the Deployment tab and select the online deployment name you previously created. Notice in the deployment details the endpoint has an unique ID and the deployment is pointing to a specific model asset (in this case the Pipeline 3 model). Optionally, feel free to test the model by submitting a sample payload in the test panel as you did in the model deployment lab . Go back to the deployment space overview by clicking on the deployment space name in the navigation breadcrumbs on the top left. Now, on the Deployments tab. Click on the three vertical dots to the right of your online deployment. Then select the Replace asset option from the menu. You can now select the asset that will replace the existing model for this deployment. Again, one thing to note is that you should only select an asset that is using a framework compatible with the existing deployed model and that the input schema exists and matches between the new and deployed model. Otherwise, the deployment may fail. Click the checkbox next to the second model you promoted to this space (in this case, Pipeline 8). Then click the Replace button. The deployment will be updated to use the replaced model. This may take a few minutes, during which time you will see the status set to Updating . Once the update completes, you will receive a notification that the deployment has been updated and the status will change to Deployed . To validate the update, click on the deployment name. Notice that the endpoint has not changed (the unique id is still the same) but the associated asset has been updated to our new model. Optionally, feel free to test the model by submitting a sample payload in the test panel as you did in the model deployment lab . You may notice that the model makes a different prediction or have different scores from the previous model.","title":"Modify the Online Deployment"},{"location":"ml-model-ops-and-versioning/deployment-model-updates/#extra-instructions-to-save-and-promote-a-second-model","text":"These additional instructions are provided to help you deploy a second model from an existing AutoAI experiment. You only need to run them if you do not currently have two models in your deployment space and you need a reminder on how to save and promote another pipeline from AutoAI, which you need to complete the lab above. To get started, we will need to identify the new model we want to use. Go the (\u2630) navigation menu, expand Projects and then click on your analytics project. Click on the completed AutoAI experiment you previously ran. Scroll down to see the Pipeline leaderboard . In this case, Pipeline 3 was the pipeline with the best result (on the cross validation set) for our experiment and the one we deployed during the previous lab. We will deploy a second pipeline using a different model algorithm. Click on the Save as button next for one of other pipelines. In this case, we are saving Pipeline 8. Note: The exact pipelines in your experiment may be different. The goal of this portion of the lab is to showcase how a deployment can be updated, so we can save any of the pipelines we have not previously promoted. Choose the Model tile, accept the default name or change it if you like. Add optional description or tags, and click Create . You will see a notification to indicate that your model is saved to your project. Click the View in project link in the notification to go to the saved model. ( Alternatively, if you navigate back to your project assets tab by closing the pipeline and AutoAI experiment, you will see the saved model in the Models section, which you can click on to explore ). To make the model available to be deployed, we need to make it available in the deployment space you created during the setup module. Click on the Promote to deployment space button. Select the deployment space that was created as part of the setup module as the Target space and click Promote . You will be brought back to your project assets page and see a notification that the model was promoted to the deployment space succesfully. Feel free to close that notification.","title":"Extra Instructions to Save and Promote a Second Model"},{"location":"ml-model-ops-and-versioning/deployment-model-updates/#conclusion","text":"In this section we covered one approach to updating the deployment of a machine learning model. Although there are other aspects to managing and operating machine learning models in production, we have seen how to make changes to a model without impacting the model serving endpoint.","title":"Conclusion"},{"location":"setup/","text":"Workshop Setup \u00b6 Before we get started with the workshop, you will need to download some assets and setup your environment. This section is broken up into the following steps: Download Assets Create IBM Cloud Account and IBM Cloud Pak for Data services Create a Project and Deployment Space Get the IBM Cloud platform API key Conclusion Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Download Assets \u00b6 Various parts of this workshop will require the attendee to upload files or run scripts. These artifacts have been collected in the following two zip files which you can download using the links below. For each line below, click on the [Download] link to get the file. If the link isn't working for you, try clicking the [Mirror] to get it from a backup server. You'll need these files in the next sections. Cloud Pak for Data Project - [Download] | [Mirror] Sample Python Application - [Download] | [Mirror] Create IBM Cloud Account and Services \u00b6 We need to provision our Cloud Pak for Data as a Service instance. Cloud Pak for Data provides you with an integrated set of capabilities for collecting and organizing your data into a trusted, unified view, and then creating and scaling AI models across your business. Create Cloud Pak for Data Services \u00b6 Launch a new web browser window or tab and navigate to IBM Cloud Pak for Data using the region closest to your location from the list below: US, Dallas EU, Frankfurt Japan, Tokyo You can leave the pre-selected region or select the region nearest to you. Next, log in by doing one of the following: If you do not have an IBMid, enter your email address on the left panel and accept the terms checkbox in the Create a new IBM Cloud Account section. Then click the Next button to complete the process of creating a new account. If you already have and IBMid, click on the Log in with your IBMid link. Enter the requested profile information and then click the Continue button. Note: If you are a returning user and you have watson services in a different region than the pre-selected one, you will see an error message telling you to select that region instead. See the FAQ section for help. The services required for IBM Cloud Pak for Data will be automatically provisioned for you. Once you see a message that says that the apps are ready to use, click on Go to IBM Cloud Pak for Data . Verify Service Instances \u00b6 Click on the (\u2630) navigation menu on the top left corner of the Cloud Pak for Data UI. Expand Services and then click on Service instances . If you see an instance of Machine Learning , take note of its name and you can skip to section Create a Project and Deployment Space . If you do not have an instance of Machine Learning , click on the Add service + button. Search or scroll until you find the tile for Machine Learning and click on it. In the 'Select a region' drop down, choose the same region as you chose for your Cloud Pak for Data as a Service platform. Select the Free tier in the 'Pricing plan' section. Optionally, scroll down and change the name of the instance. Finally, click the Create button. Note: If you have any issues creating the services, please see the FAQ section for help. Create a Project and Deployment Space \u00b6 Import the Project \u00b6 In Cloud Pak for Data, we use the concept of a project to collect / organize the resources used to achieve a particular goal (resources to build a solution to a problem). Your project resources can include data, collaborators, and analytic assets like notebooks and models, etc. Go the (\u2630) navigation menu, expand Projects and click on the View all projects link. Click on the New project button. If you have existing projects, your screen will look different, click on the New + option on the top right. We are going to create a project from an existing file (which contains the assets we will use throughout this workshop), as opposed to creating an empty project. Select the Create a project from a sample or file option. Click on the browse link and in the file browser popup, navigate to where you downloaded the files for this lab. Then select the DDC2021-Course-DevToProductionProject.zip file. Give the project a name. If this is the first time you are creating a project and you do not have an IBM Cloud Object Storage (ICOS) service instance, you will see the Add link in the Define Storage section. If you already have a cloud object storage instance populated in this section, skip to the next step. Otherwise, provision an ICOS instance: Go ahead and click on the Add link to create an instance (if you already have an object storage displayed, proceed to the next step). A new browser tab will open up, where you can create the Cloud Object Service. By default, a Lite (Free) plan will be selected. Scroll down and update the name of your Cloud Object Storage service if you wish, and click Create . The browser tab will automatically close when the Cloud Object Storage instance has been created. Back on IBM Cloud Pak for Data as a Service, click Refresh . Note: If you don't see the object storage instance, click the Refresh option again. Your Cloud Object Storage instance will be displayed under \"Storage\". Click Create to finish creating the project. You can see a progress bar that says your project is being created. Once the project is succesfully created, on the pop up window click on the View new project button. Clicking on the Assets tab will show all the assets that were imported into the project when it was created. Associate a Watson Machine Learning Service instance to the project \u00b6 You will need to associate a Watson Machine Learning service instance to your project in order to run Machine Learning experiments. Go to the Settings tab of your project and look for the Associated services section. Click on Add service and in the menu that opens up, click on Watson . Click the checkbox next to the Watson Machine Learning service instance that was created for you when you signed up for Cloud Pak for Data as a Service or the one you created in section 2. Click Associate service . Note: If you have multiple WatsonMachineLearning services, make sure you select the one that is in the same regions as is your Cloud Pak for Data as a service. You willsee a notification that the WatsonMachineLearning service was successfully associated with your project. Click on the X in the right top corner to close the pop up modal and go back to your project. Create a Deployment Space \u00b6 Cloud Pak for Data uses the concept of Deployment Spaces to configure and manage the deployment of a set of related deployable assets. These assets can be data files, machine learning models, etc. For this workshop, we need to create one. Go the (\u2630) navigation menu, expand Deployments and then select View all spaces . Click on the New deployment space button. Give your deployment space a unique name and optional description. In the service drop downs, select the Cloud Object Storage instance that you had created when you were creating the project and select the Machine Learning Service instance associated with your IBM Cloud Pak for Data as a Service instance. Then click the Create button. Once the deployment space is created, you can click on View new space . Get API Access Details \u00b6 In some parts of this workshop, you will be using the Watson Machine Learning (WML) SDK / APIs to perform operations on your Watson Machine Learning instance. To programmatically access your Watson Machine Learning instance, you will need to provide the API key for your IBM Cloud account as well as the location of the WML service instance. Get an API Key \u00b6 You will use the IBM Cloud Console to generate the IBM Cloud API key. In a new browser window or tab, open the API keys section of the IBM Cloud console link . Select My IBM Cloud API keys in the View dropdown and then click Create an IBM Cloud API key + . Give your API key a unique name and click Create . You should see a message that says API key successfully created . Click Copy to copy the generated API key and save it locally as you will need it in the workshop labs. Get the WML Service Instance Location \u00b6 You will need to know the location (i.e region code) where your machine learning service instance is provisioned. If you know the region where you provisioned the service, you can determine the region code from the table below: Region Region Codes Dallas us-south Tokyo jp-tok London eu-gb Frankfurt eu-de If you are not sure of the region you provisioned, you can use the IBM Cloud CLI to obtain the location of the machine learning service instance. In a new browser window or tab, go to the IBM Cloud Home Page and click the terminal icon in the upper right-hand bar to launch a new cloud shell web terminal window. Wait for the web terminal to be ready and then run the following command to retrieve information about the Machine Learning service instance. Remember to replace <WML_INSTANCE_NAME> with the name of your Machine Learning instance associated with your IBM Cloud Pak for Data as a Service instance. ibmcloud resource service-instance <WML_INSTANCE_NAME> Note: The <WML_INSTANCE_NAME> is the name of your machine learning instance name, which we saw in the 'Verify Service Instances' section above. Get the value of Location from this result. This is the value that you will need to save for future labs. Conclusion \u00b6 We have now completed creating an IBM Cloud account, a Cloud Pak for Data as a Service instance, and the project and deployment space that we will use in the rest of this workshop. We have also obtained the IBM Cloud API key that we will use to invoke APIs for your services.","title":"Environment Setup"},{"location":"setup/#workshop-setup","text":"Before we get started with the workshop, you will need to download some assets and setup your environment. This section is broken up into the following steps: Download Assets Create IBM Cloud Account and IBM Cloud Pak for Data services Create a Project and Deployment Space Get the IBM Cloud platform API key Conclusion Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page.","title":"Workshop Setup"},{"location":"setup/#download-assets","text":"Various parts of this workshop will require the attendee to upload files or run scripts. These artifacts have been collected in the following two zip files which you can download using the links below. For each line below, click on the [Download] link to get the file. If the link isn't working for you, try clicking the [Mirror] to get it from a backup server. You'll need these files in the next sections. Cloud Pak for Data Project - [Download] | [Mirror] Sample Python Application - [Download] | [Mirror]","title":"Download Assets"},{"location":"setup/#create-ibm-cloud-account-and-services","text":"We need to provision our Cloud Pak for Data as a Service instance. Cloud Pak for Data provides you with an integrated set of capabilities for collecting and organizing your data into a trusted, unified view, and then creating and scaling AI models across your business.","title":"Create IBM Cloud Account and Services"},{"location":"setup/#create-cloud-pak-for-data-services","text":"Launch a new web browser window or tab and navigate to IBM Cloud Pak for Data using the region closest to your location from the list below: US, Dallas EU, Frankfurt Japan, Tokyo You can leave the pre-selected region or select the region nearest to you. Next, log in by doing one of the following: If you do not have an IBMid, enter your email address on the left panel and accept the terms checkbox in the Create a new IBM Cloud Account section. Then click the Next button to complete the process of creating a new account. If you already have and IBMid, click on the Log in with your IBMid link. Enter the requested profile information and then click the Continue button. Note: If you are a returning user and you have watson services in a different region than the pre-selected one, you will see an error message telling you to select that region instead. See the FAQ section for help. The services required for IBM Cloud Pak for Data will be automatically provisioned for you. Once you see a message that says that the apps are ready to use, click on Go to IBM Cloud Pak for Data .","title":"Create Cloud Pak for Data Services"},{"location":"setup/#verify-service-instances","text":"Click on the (\u2630) navigation menu on the top left corner of the Cloud Pak for Data UI. Expand Services and then click on Service instances . If you see an instance of Machine Learning , take note of its name and you can skip to section Create a Project and Deployment Space . If you do not have an instance of Machine Learning , click on the Add service + button. Search or scroll until you find the tile for Machine Learning and click on it. In the 'Select a region' drop down, choose the same region as you chose for your Cloud Pak for Data as a Service platform. Select the Free tier in the 'Pricing plan' section. Optionally, scroll down and change the name of the instance. Finally, click the Create button. Note: If you have any issues creating the services, please see the FAQ section for help.","title":"Verify Service Instances"},{"location":"setup/#create-a-project-and-deployment-space","text":"","title":"Create a Project and Deployment Space"},{"location":"setup/#import-the-project","text":"In Cloud Pak for Data, we use the concept of a project to collect / organize the resources used to achieve a particular goal (resources to build a solution to a problem). Your project resources can include data, collaborators, and analytic assets like notebooks and models, etc. Go the (\u2630) navigation menu, expand Projects and click on the View all projects link. Click on the New project button. If you have existing projects, your screen will look different, click on the New + option on the top right. We are going to create a project from an existing file (which contains the assets we will use throughout this workshop), as opposed to creating an empty project. Select the Create a project from a sample or file option. Click on the browse link and in the file browser popup, navigate to where you downloaded the files for this lab. Then select the DDC2021-Course-DevToProductionProject.zip file. Give the project a name. If this is the first time you are creating a project and you do not have an IBM Cloud Object Storage (ICOS) service instance, you will see the Add link in the Define Storage section. If you already have a cloud object storage instance populated in this section, skip to the next step. Otherwise, provision an ICOS instance: Go ahead and click on the Add link to create an instance (if you already have an object storage displayed, proceed to the next step). A new browser tab will open up, where you can create the Cloud Object Service. By default, a Lite (Free) plan will be selected. Scroll down and update the name of your Cloud Object Storage service if you wish, and click Create . The browser tab will automatically close when the Cloud Object Storage instance has been created. Back on IBM Cloud Pak for Data as a Service, click Refresh . Note: If you don't see the object storage instance, click the Refresh option again. Your Cloud Object Storage instance will be displayed under \"Storage\". Click Create to finish creating the project. You can see a progress bar that says your project is being created. Once the project is succesfully created, on the pop up window click on the View new project button. Clicking on the Assets tab will show all the assets that were imported into the project when it was created.","title":"Import the Project"},{"location":"setup/#associate-a-watson-machine-learning-service-instance-to-the-project","text":"You will need to associate a Watson Machine Learning service instance to your project in order to run Machine Learning experiments. Go to the Settings tab of your project and look for the Associated services section. Click on Add service and in the menu that opens up, click on Watson . Click the checkbox next to the Watson Machine Learning service instance that was created for you when you signed up for Cloud Pak for Data as a Service or the one you created in section 2. Click Associate service . Note: If you have multiple WatsonMachineLearning services, make sure you select the one that is in the same regions as is your Cloud Pak for Data as a service. You willsee a notification that the WatsonMachineLearning service was successfully associated with your project. Click on the X in the right top corner to close the pop up modal and go back to your project.","title":"Associate a Watson Machine Learning Service instance to the project"},{"location":"setup/#create-a-deployment-space","text":"Cloud Pak for Data uses the concept of Deployment Spaces to configure and manage the deployment of a set of related deployable assets. These assets can be data files, machine learning models, etc. For this workshop, we need to create one. Go the (\u2630) navigation menu, expand Deployments and then select View all spaces . Click on the New deployment space button. Give your deployment space a unique name and optional description. In the service drop downs, select the Cloud Object Storage instance that you had created when you were creating the project and select the Machine Learning Service instance associated with your IBM Cloud Pak for Data as a Service instance. Then click the Create button. Once the deployment space is created, you can click on View new space .","title":"Create a Deployment Space"},{"location":"setup/#get-api-access-details","text":"In some parts of this workshop, you will be using the Watson Machine Learning (WML) SDK / APIs to perform operations on your Watson Machine Learning instance. To programmatically access your Watson Machine Learning instance, you will need to provide the API key for your IBM Cloud account as well as the location of the WML service instance.","title":"Get API Access Details"},{"location":"setup/#get-an-api-key","text":"You will use the IBM Cloud Console to generate the IBM Cloud API key. In a new browser window or tab, open the API keys section of the IBM Cloud console link . Select My IBM Cloud API keys in the View dropdown and then click Create an IBM Cloud API key + . Give your API key a unique name and click Create . You should see a message that says API key successfully created . Click Copy to copy the generated API key and save it locally as you will need it in the workshop labs.","title":"Get an API Key"},{"location":"setup/#get-the-wml-service-instance-location","text":"You will need to know the location (i.e region code) where your machine learning service instance is provisioned. If you know the region where you provisioned the service, you can determine the region code from the table below: Region Region Codes Dallas us-south Tokyo jp-tok London eu-gb Frankfurt eu-de If you are not sure of the region you provisioned, you can use the IBM Cloud CLI to obtain the location of the machine learning service instance. In a new browser window or tab, go to the IBM Cloud Home Page and click the terminal icon in the upper right-hand bar to launch a new cloud shell web terminal window. Wait for the web terminal to be ready and then run the following command to retrieve information about the Machine Learning service instance. Remember to replace <WML_INSTANCE_NAME> with the name of your Machine Learning instance associated with your IBM Cloud Pak for Data as a Service instance. ibmcloud resource service-instance <WML_INSTANCE_NAME> Note: The <WML_INSTANCE_NAME> is the name of your machine learning instance name, which we saw in the 'Verify Service Instances' section above. Get the value of Location from this result. This is the value that you will need to save for future labs.","title":"Get the WML Service Instance Location"},{"location":"setup/#conclusion","text":"We have now completed creating an IBM Cloud account, a Cloud Pak for Data as a Service instance, and the project and deployment space that we will use in the rest of this workshop. We have also obtained the IBM Cloud API key that we will use to invoke APIs for your services.","title":"Conclusion"},{"location":"trusted-ai/","text":"Trusted AI - Fairness in Machine Learning \u00b6 In this module, you will explore aspects of trust and transparency in AI. As machine learning models are built and used in production scenarios, it is essential that we can trust the output of these models/pipelines/systems. To build that trust we need a way to ensure the AI models/systems are reliable, that they are fair, that they are secure, that the predictions being made can be understood, and that they are robust. In this section, we look specifically at fairness in AI models (and the data used to build the models) to prevent bias. Bias in AI can occur when models give preferred outcomes to certain groups (inadvertently discriminating against race, or perpetuating gender bias, etc). For the credit risk example used in the workshop, bias in the models can mean that certain populations of customers may be harmed or treated unfairly (i.e not receive loans). And the organizations using those models may be subject to legal and/or financial penalties. In the lab, we will be using the AI Fairness 360 (AIF360) Toolkit. AIF360 is an extensible open source toolkit with over 70 fairness metrics and 10 state-of-the-art bias mitigation algorithm, which can be used to help examine and mitigate bias throughout the AI model/application lifecycle. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. Open Bias Exploration and Mitigation Notebook \u00b6 We'll be using a Jupyter notebook in this lab which contains the code to load a dataset, then explore a small subset of the bias metrics and bias mitigation strategies from AIF360. The Jupyter notebook is already included as an asset in the project you imported during the setup module. Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the bias-mitigation notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section. Run Jupyter notebook \u00b6 Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ([*]) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17]). The notebook generated is pre filled with Python code and is divided into the following sections: 1.0 Definitions : There are a number of definitions for bias. We will explore bias in the context of machine learning and specifically the bias that can cause unfairness in AI. 2.0 Setup : This section installs the necessary packages and libraries to run the AIF360 toolkit and imports the specifc metrics / algorithims we will explore. After the libraries are installed, you should restart the Jupyter kernel. Click on Kernel menu at the top of the page and select the Restart option from the menu. 3.0 Load and Explore Data : This section of cells will import the German Credit risk dataset we have been using through the other modules. In this lab, we are using the dataset directly from the toolkit to save some time since it is already in the format expected by the metrics and mitigation algorithms. 4.0 Exploring Bias with AIF360 : With the data loaded, we can start to explore bias for both the specific dataset and the machine learning model we aim to build from the data. For this dataset, we know bias can occur based on age or gender. The notebook will focus on the age attribute and explore the following (very small subset of bias metrics): Dataset: Given its ultimately a binary target in the training data, we explore bias for the training and test data using the Binary Label Dataset metric. We should see there is some disparate impact in the dataset based on age. Model: We will build a simple ML model to highlight how we can also explore bias in the predictions made by a model. Ultimately, this is a classification model, so we will use the Classification Metric . 5.0 AI Fairness Algorithms : Now that we know there is bias in our training data and the model, this final section in the notebook explores mitigation approaches. The notebook will only focus on the pre-processing algorithm. In specific, the notebook will look at the Reweighing algorithm which will transform the dataset to have more equity in positive outcomes on the protected attribute for the privileged and unprivileged groups. Stop the Environment \u00b6 In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window. Conclusion \u00b6 Congratulations. You've completed this lab and now have a basic understanding of how to install the AIF360 toolkit and some practical experience using a small subset of the toolkit to calculate bias and mitigate it. Additional Resources / References: There are lots of resources to continue exploring Trust in AI. To start, consider exploring some of the links below: IBM Research - Fairness IBM Developer Center for Open Source Data and AI Technologies - Trusted AI Sample Notebook using AIF360 and AutoAI","title":"Bias with AIF360"},{"location":"trusted-ai/#trusted-ai-fairness-in-machine-learning","text":"In this module, you will explore aspects of trust and transparency in AI. As machine learning models are built and used in production scenarios, it is essential that we can trust the output of these models/pipelines/systems. To build that trust we need a way to ensure the AI models/systems are reliable, that they are fair, that they are secure, that the predictions being made can be understood, and that they are robust. In this section, we look specifically at fairness in AI models (and the data used to build the models) to prevent bias. Bias in AI can occur when models give preferred outcomes to certain groups (inadvertently discriminating against race, or perpetuating gender bias, etc). For the credit risk example used in the workshop, bias in the models can mean that certain populations of customers may be harmed or treated unfairly (i.e not receive loans). And the organizations using those models may be subject to legal and/or financial penalties. In the lab, we will be using the AI Fairness 360 (AIF360) Toolkit. AIF360 is an extensible open source toolkit with over 70 fairness metrics and 10 state-of-the-art bias mitigation algorithm, which can be used to help examine and mitigate bias throughout the AI model/application lifecycle. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space.","title":"Trusted AI - Fairness in Machine Learning"},{"location":"trusted-ai/#open-bias-exploration-and-mitigation-notebook","text":"We'll be using a Jupyter notebook in this lab which contains the code to load a dataset, then explore a small subset of the bias metrics and bias mitigation strategies from AIF360. The Jupyter notebook is already included as an asset in the project you imported during the setup module. Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. From your Project overview page, click on the Assets tab to open the assets page where your project assets are stored and organized. Scroll down to the Notebooks section of the page and click on the pencil icon at the right of the bias-mitigation notebook. When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.","title":"Open Bias Exploration and Mitigation Notebook"},{"location":"trusted-ai/#run-jupyter-notebook","text":"Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the Run button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ([*]) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17]). The notebook generated is pre filled with Python code and is divided into the following sections: 1.0 Definitions : There are a number of definitions for bias. We will explore bias in the context of machine learning and specifically the bias that can cause unfairness in AI. 2.0 Setup : This section installs the necessary packages and libraries to run the AIF360 toolkit and imports the specifc metrics / algorithims we will explore. After the libraries are installed, you should restart the Jupyter kernel. Click on Kernel menu at the top of the page and select the Restart option from the menu. 3.0 Load and Explore Data : This section of cells will import the German Credit risk dataset we have been using through the other modules. In this lab, we are using the dataset directly from the toolkit to save some time since it is already in the format expected by the metrics and mitigation algorithms. 4.0 Exploring Bias with AIF360 : With the data loaded, we can start to explore bias for both the specific dataset and the machine learning model we aim to build from the data. For this dataset, we know bias can occur based on age or gender. The notebook will focus on the age attribute and explore the following (very small subset of bias metrics): Dataset: Given its ultimately a binary target in the training data, we explore bias for the training and test data using the Binary Label Dataset metric. We should see there is some disparate impact in the dataset based on age. Model: We will build a simple ML model to highlight how we can also explore bias in the predictions made by a model. Ultimately, this is a classification model, so we will use the Classification Metric . 5.0 AI Fairness Algorithms : Now that we know there is bias in our training data and the model, this final section in the notebook explores mitigation approaches. The notebook will only focus on the pre-processing algorithm. In specific, the notebook will look at the Reweighing algorithm which will transform the dataset to have more equity in positive outcomes on the protected attribute for the privileged and unprivileged groups.","title":"Run Jupyter notebook"},{"location":"trusted-ai/#stop-the-environment","text":"In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done. Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page. Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the Tool value is Notebook ). Click on the three vertical dots at the right of that row and select the Stop option from the menu. Click the Stop button on the subsequent pop up window.","title":"Stop the Environment"},{"location":"trusted-ai/#conclusion","text":"Congratulations. You've completed this lab and now have a basic understanding of how to install the AIF360 toolkit and some practical experience using a small subset of the toolkit to calculate bias and mitigate it. Additional Resources / References: There are lots of resources to continue exploring Trust in AI. To start, consider exploring some of the links below: IBM Research - Fairness IBM Developer Center for Open Source Data and AI Technologies - Trusted AI Sample Notebook using AIF360 and AutoAI","title":"Conclusion"}]}